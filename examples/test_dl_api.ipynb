{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96e58e7-51d0-486f-85ad-51dc18735e15",
   "metadata": {},
   "source": [
    "# Scratch Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06aa15c-dec6-483d-bfca-2a97a4c7fe65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8320caab-95e5-484b-985f-573c91132408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "|5.0|\n",
      "|6.0|\n",
      "|7.0|\n",
      "|8.0|\n",
      "|9.0|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('a', DoubleType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[float(x)] for x in range(10)]\n",
    "df = spark.createDataFrame(\n",
    "    data, schema=[\"a\"]\n",
    ")\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ed95f2-a46e-4c87-bad5-d00a6eada3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 10:32:04 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 869, in main\n",
      "    process()\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 861, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 354, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 347, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 555, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 821, in predict\n",
      "    yield _validate_and_transform_prediction_result(\n",
      "  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 333, in _validate_and_transform_prediction_result\n",
      "    raise ValueError(\"Prediction results must have same length as input data.\")\n",
      "ValueError: Prediction results must have same length as input data.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/04/26 10:32:04 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 869, in main\n    process()\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 861, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 354, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 347, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 555, in func\n    for result_batch, result_type in result_iter:\n  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 821, in predict\n    yield _validate_and_transform_prediction_result(\n  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 333, in _validate_and_transform_prediction_result\n    raise ValueError(\"Prediction results must have same length as input data.\")\nValueError: Prediction results must have same length as input data.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict\n\u001b[1;32m      9\u001b[0m embedding \u001b[38;5;241m=\u001b[39m predict_batch_udf(\n\u001b[1;32m     10\u001b[0m     make_embedding_fn,\n\u001b[1;32m     11\u001b[0m     return_type\u001b[38;5;241m=\u001b[39mArrayType(FloatType()),\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:929\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    924\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    925\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    926\u001b[0m     )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/errors/exceptions/captured.py:182\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 869, in main\n    process()\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 861, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 354, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/home/leey/devpub/spark/python/pyspark/sql/pandas/serializers.py\", line 347, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/leey/devpub/spark/python/pyspark/worker.py\", line 555, in func\n    for result_batch, result_type in result_iter:\n  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 821, in predict\n    yield _validate_and_transform_prediction_result(\n  File \"/home/leey/devpub/spark/python/pyspark/ml/functions.py\", line 333, in _validate_and_transform_prediction_result\n    raise ValueError(\"Prediction results must have same length as input data.\")\nValueError: Prediction results must have same length as input data.\n"
     ]
    }
   ],
   "source": [
    "def make_embedding_fn():\n",
    "    def predict(a: np.ndarray) -> np.ndarray:\n",
    "        print(f\">>> {a=}\")\n",
    "        output = a * np.array([1.0, 2.0, 3.0])\n",
    "        print(f\">>> {output=}\")\n",
    "        return output\n",
    "    return predict\n",
    "\n",
    "embedding = predict_batch_udf(\n",
    "    make_embedding_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "df.select(embedding(\"a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ac9c4-bc91-411d-b4e2-df61647df06f",
   "metadata": {},
   "source": [
    "## Create pandas/spark dataframes for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2657a5e-ff67-44b9-8205-a7f00db8ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import array, col, pandas_udf, spark_partition_id, struct\n",
    "from pyspark.sql.types import *\n",
    "from typing import Iterator, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e58d6b-f00e-48ad-8af1-7c5b3805c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 100000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e016b1-1ea6-4676-9b86-dd191eb969e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_scalar = pdf\n",
    "df_scalar = spark.createDataFrame(pdf_scalar)\n",
    "\n",
    "pdf_tensor = pd.DataFrame()\n",
    "pdf_tensor['t1'] = pdf_scalar.values.tolist()\n",
    "df_tensor1 = spark.createDataFrame(pdf_tensor)\n",
    "\n",
    "pdf_tensor['t2'] = pdf_scalar.drop(columns='d').values.tolist()\n",
    "df_tensor2 = spark.createDataFrame(pdf_tensor)\n",
    "\n",
    "pdf_scalar_tensor = pdf\n",
    "pdf_scalar_tensor['t1'] = pdf.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955b40f-4efa-4236-88e1-966e150ce21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_scalar_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329ffe9-99a7-4014-8f85-1782277840a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_shapes = [None, None, None, None, [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ccee4-531a-4c19-a8bc-cf1ee309bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [pdf_scalar_tensor[col] for col in pdf_scalar_tensor.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cea74-010c-4654-9510-13f72e450515",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [np.vstack(v).reshape([-1] + input_tensor_shapes[i]) if input_tensor_shapes[i] else v for i, v in enumerate(inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb2880-fb7a-47a5-a3cc-80341bbc4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,t1 = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3149-bd61-4d45-b667-cbfa01ff8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(t1, axis=1) + a + b + c + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4441eb-5a68-4744-bc14-4942ef7f2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_shapes = {4:[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f5d65-904b-4d9c-adb8-8af8b9a742a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = [None] * 5\n",
    "for index, shape in input_tensor_shapes.items():\n",
    "    input_shapes[index] = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2b29a-9de3-4ed5-9bba-697bb621f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5f130-7074-48ca-8e0c-0bdc7e6b5b5b",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619fdce-350b-4c88-b3bc-9fe31a91558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "\n",
    "# 4 scalar columns\n",
    "pdf = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\", \"d\"])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb864-b3a1-4938-b328-f4f6e496aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9c546-0ca4-4f86-81c3-dac6aa7d3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiples_with_array_fn():\n",
    "    def predict(x, y):\n",
    "        return {\"x2\": x * 2, \"y3\": y * 3}\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008efff-d4d4-4843-a683-8b66f656eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiples_w_array = predict_batch_udf(\n",
    "    multiples_with_array_fn,\n",
    "    return_type=StructType(\n",
    "        [StructField(\"x2\", DoubleType(), True), StructField(\"y3\", ArrayType(DoubleType()), True)]\n",
    "    ),\n",
    "    input_tensor_shapes=[[], [3]],\n",
    "    batch_size=5,\n",
    ")\n",
    "preds = df.withColumn(\"preds\", multiples_w_array(\"a\", array([\"b\", \"c\", \"d\"]))).select(\"a\", \"preds.*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91522a3-a2ee-4121-a3da-eefebb15fb43",
   "metadata": {},
   "source": [
    "### pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe674677-acc5-4827-bb86-5c370999a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3c00b-0f6a-48ab-a215-ce2c0467f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000a092-e40b-4b96-8f12-b69466ca04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pdf_scalar['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02ea27-6328-4df9-a1c6-2064c59c683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdf79e-3d12-4c7c-b0ab-02845cc54d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = pdf_scalar['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e444e7f-f48b-45d3-9fbf-274d9cd5e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.arange(0, 4*100000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22483bc-ec80-437d-bc88-0d0bed15dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(df, n=1):\n",
    "    for _, batch in df.groupby(np.arange(len(df)) // n):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf803d9-bfec-4448-9db6-e746d3e9bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch1(df, n=1):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a170c15-e144-44ce-bc99-71fae5f24bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2(df, n=1):\n",
    "    index = 0\n",
    "    data_size = len(df)\n",
    "    while index < data_size:\n",
    "        yield df.iloc[index : index + n]\n",
    "        index += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197eb112-a8cd-4624-af60-0b267161c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "for x in batch(pdf, 1000):\n",
    "    x = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be799c-570f-437d-b012-34a8f0c5af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "for x in batch1(pdf, 1000):\n",
    "    x = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02149b-7fdc-42a8-bde7-91bd49c1e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "for x in batch2(pdf, 1000):\n",
    "    x = x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5b156-b62d-494a-a3cc-9d75b9e37fae",
   "metadata": {},
   "source": [
    "## Pydoc tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21538c08-803b-43cb-98c2-dd53063dfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField\n",
    "from typing import Mapping\n",
    "\n",
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])\n",
    "pdf_tensor = pd.DataFrame()\n",
    "pdf_tensor['t1'] = pdf.values.tolist()\n",
    "pdf_tensor['t2'] = pdf.drop(columns='d').values.tolist()\n",
    "df = spark.createDataFrame(pdf_tensor)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900311b8-043c-40e4-a8ce-9d8bfc4dae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_sum_fn():\n",
    "    def predict(x1: np.ndarray, x2: np.ndarray) -> Mapping[str, np.dtype]:\n",
    "        # x1.shape = [batch_size, 4]\n",
    "        # x2.shape = [batch_size, 3]\n",
    "        result = np.sum(x1, axis=1) + np.sum(x2, axis=1)\n",
    "        # result.shape = [batch_size], result_type = FloatType()\n",
    "        return result\n",
    "\n",
    "    return predict\n",
    "\n",
    "# multiple tensor columns with tensor_input_shapes => list of numpy arrays\n",
    "sum_cols = predict_batch_udf(\n",
    "    multi_sum_fn,\n",
    "    return_type=FloatType(),\n",
    "    batch_size=5,\n",
    "    input_tensor_shapes=[[4], [3]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fe895-f81d-4c9e-b6b0-52e2dd80f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"sum\", sum_cols(\"t1\", \"t2\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe83e79-1002-4572-baa8-a56b7066fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_sum_fn():\n",
    "    def predict_columnar(x1, x2):\n",
    "        # x1.shape = [batch_size, 4]\n",
    "        # x2.shape = [batch_size, 3]\n",
    "        return {\n",
    "            \"sum1\": np.sum(x1, axis=1),\n",
    "            \"sum2\": np.sum(x2, axis=1)\n",
    "        }  # return_type = StructType()\n",
    "\n",
    "    return predict_columnar\n",
    "\n",
    "sum_cols = predict_batch_udf(\n",
    "    make_multi_sum_fn,\n",
    "    return_type=StructType([\n",
    "        StructField(\"sum1\", FloatType(), True),\n",
    "        StructField(\"sum2\", FloatType(), True)\n",
    "    ]),\n",
    "    batch_size=5,\n",
    "    input_tensor_shapes=[[4], [3]],\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\", sum_cols(\"t1\", \"t2\")).select(\"t1\", \"t2\", \"sum.*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f28b5-fcdd-40b2-ab79-814665332849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_sum_fn():\n",
    "    def predict_row(x1: np.ndarray, x2: np.ndarray) -> list[Mapping[str, float]]:\n",
    "        # x1.shape = [batch_size, 4]\n",
    "        # x2.shape = [batch_size, 3]\n",
    "        return [{'sum1': np.sum(x1[i]), 'sum2': np.sum(x2[i])} for i in range(len(x1))]\n",
    "    return predict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b1020-9804-469f-a927-19647651538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sum_udf = predict_batch_udf(\n",
    "    make_multi_sum_fn,\n",
    "    return_type=StructType([\n",
    "        StructField(\"sum1\", FloatType(), True),\n",
    "        StructField(\"sum2\", FloatType(), True)\n",
    "    ]),\n",
    "    batch_size=5,\n",
    "    input_tensor_shapes=[[4], [3]],\n",
    ")\n",
    "\n",
    "df.withColumn(\"sum\", multi_sum_udf(\"t1\", \"t2\")).select(\"t1\", \"t2\", \"sum.*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e053185-7f6e-44cd-9a25-4be1a160c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_times_two_fn():\n",
    "    def predict(x1: np.ndarray, x2: np.ndarray) -> Mapping[str, np.ndarray]:\n",
    "        # x1.shape = [batch_size, 4]\n",
    "        # x2.shape = [batch_size, 3]\n",
    "        return {\"t1x2\": x1 * 2, \"t2x2\": x2 * 2}\n",
    "    return predict\n",
    "\n",
    "multi_times_two_udf = predict_batch_udf(\n",
    "    make_multi_times_two_fn,\n",
    "    return_type=StructType([\n",
    "        StructField(\"t1x2\", ArrayType(FloatType()), True),\n",
    "        StructField(\"t2x2\", ArrayType(FloatType()), True)\n",
    "    ]),\n",
    "    batch_size=5,\n",
    "    input_tensor_shapes=[[4], [3]],\n",
    ")\n",
    "\n",
    "df.withColumn(\"x2\", multi_times_two_udf(\"t1\", \"t2\")).select(\"t1\", \"t2\", \"x2.*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8c90b-00cb-4928-85bc-52e881a592b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbea1a-a4bf-4f84-ad44-a597b2f2443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size]\n",
    "        # outputs.shape = [batch_size]  # return_type = FloatType()\n",
    "        return inputs * 2\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_two = predict_batch_udf(predict_batch_fn,\n",
    "                              return_type=FloatType(),\n",
    "                              batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f1b07-891d-4a69-a13f-97e8034bceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"x2\", times_two(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e34f31-48af-411d-9538-149f711afd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size, 1]\n",
    "        # outputs.shape = [batch_size]   # return_type = FloatType()\n",
    "        return np.squeeze(inputs) * 3\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_three = predict_batch_udf(predict_batch_fn,\n",
    "                                return_type=FloatType(),\n",
    "                                batch_size=10)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.withColumn(\"x3\", times_three(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c0e36-b85f-4837-8fdb-b83f521ad018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543b371-d317-4f56-8db0-08b1f7dc531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d869cc-c3f2-4b88-b4ba-117bf7889894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca30d3-256b-45af-8553-56ba9f3ee77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size]\n",
    "        # outputs.shape = [batch_size]  # return_type = FloatType()\n",
    "        return inputs * 2\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_two = predict_batch_udf(predict_batch_fn,\n",
    "                              return_type=FloatType(),\n",
    "                              batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d4df0-367e-4e14-9a63-73898ea56484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"x2\", times_two(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180931c1-eba1-40c0-aec6-394a819ce116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size, 1]\n",
    "        # outputs.shape = [batch_size]   # return_type = FloatType()\n",
    "        return np.squeeze(inputs) * 3\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_three = predict_batch_udf(predict_batch_fn,\n",
    "                                return_type=FloatType(),\n",
    "                                batch_size=10)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.withColumn(\"x3\", times_three(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911ab10-3b90-49c0-b348-ac0404df79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48993d4d-53b0-45b0-9b56-33a7ebefb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccd8f0-20bb-4710-a8b2-0614bc3d1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5183a-759b-40bf-970e-e4ef0ba97080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size]\n",
    "        # outputs.shape = [batch_size]  # return_type = FloatType()\n",
    "        return inputs * 2\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_two = predict_batch_udf(predict_batch_fn,\n",
    "                              return_type=FloatType(),\n",
    "                              batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a2a83-a57b-46fe-ac4f-33aad3cc92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"x2\", times_two(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8155d-0d15-4bcd-89ac-46fd41f9a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size, 1]\n",
    "        # outputs.shape = [batch_size]   # return_type = FloatType()\n",
    "        return np.squeeze(inputs) * 3\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_three = predict_batch_udf(predict_batch_fn,\n",
    "                                return_type=FloatType(),\n",
    "                                batch_size=10)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.withColumn(\"x3\", times_three(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b93991-4ae9-4ab2-8148-8d7120c47f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225e264-3d55-4f13-b339-a44a28d980fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f075b1a-c1e7-45ac-b155-537a186fae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf42af-6170-4051-9eab-6dfd1ed344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size]\n",
    "        # outputs.shape = [batch_size]  # return_type = FloatType()\n",
    "        return inputs * 2\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_two = predict_batch_udf(predict_batch_fn,\n",
    "                              return_type=FloatType(),\n",
    "                              batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e229795-29e6-490b-96ef-20ee919a1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"x2\", times_two(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d40999-74ff-4075-b233-91ba86d4d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs.shape = [batch_size, 1]\n",
    "        # outputs.shape = [batch_size]   # return_type = FloatType()\n",
    "        return np.squeeze(inputs) * 3\n",
    "\n",
    "    return predict\n",
    "\n",
    "times_three = predict_batch_udf(predict_batch_fn,\n",
    "                                return_type=FloatType(),\n",
    "                                batch_size=10)\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(np.arange(100)))\n",
    "df.withColumn(\"x3\", times_three(\"0\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed79f8-9e6f-412f-8450-a939890f1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d15f0e-0879-4c57-908f-763615c9cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "pdf = pd.DataFrame(data, columns=['a','b','c','d'])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d5023-e694-4f3f-a1a6-7d046c8223da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f157c3-fc75-4e04-8962-2d1d842a2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_batch_fn():\n",
    "        def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "            # inputs.shape = [batch_size, 4]\n",
    "            # outputs.shape = [batch_size]   # return_type = FloatType()\n",
    "            return np.sum(inputs, axis=1)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    sum = predict_batch_udf(predict_batch_fn,\n",
    "                            return_type=FloatType(),\n",
    "                            batch_size=10,\n",
    "                            input_tensor_shapes=[[4]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcd97b-d33f-4121-95a4-27d353c56d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "    sum_rows = predict_batch_udf(predict_batch_fn,\n",
    "                                 return_type=FloatType(),\n",
    "                                 batch_size=10,\n",
    "                                 input_tensor_shapes=[[4]])\n",
    "\n",
    "    df.withColumn(\"sum\", sum_rows(struct(\"a\", \"b\", \"c\", \"d\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5988e-fe24-404d-960e-61b007adbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_batch_fn():\n",
    "        def predict(x1: np.ndarray, x2: np.ndarray, x3: np.ndarray, x4: np.ndarray) -> np.ndarray:\n",
    "            # xN.shape = [batch_size, 1]\n",
    "            # outputs.shape = [batch_size, 1]   # return_type = ArrayType(FloatType())\n",
    "            print(\"x1.shape: {}\".format(x1.shape))\n",
    "            return x1 + x2 + x3 + x4\n",
    "\n",
    "        return predict\n",
    "    \n",
    "    sum_rows = predict_batch_udf(predict_batch_fn,\n",
    "                                 return_type=FloatType(),\n",
    "                                 batch_size=10)\n",
    "\n",
    "    df.withColumn(\"sum\", sum_rows(struct(\"a\", \"b\", \"c\", \"d\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1ce6c-cdeb-4038-8ed9-2342d271b3a4",
   "metadata": {},
   "source": [
    "### spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69d10c-2c7f-485b-9610-bdfa3bd22e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e972fc7-4704-4851-93f0-697cafb058ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658892b-d0ee-49fa-8d0d-06c1d96f70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8bf27-59b2-4828-a842-b03fbc7cd260",
   "metadata": {},
   "source": [
    "### spark partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada81fd-e88c-4f2f-807b-b827ae804b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "df.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad360fb-c816-4026-b906-3d0d95ef454d",
   "metadata": {},
   "source": [
    "## Test tensor columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b577952-9edc-4463-bc2c-71b29d125d4c",
   "metadata": {},
   "source": [
    "### pDF -> pDF | returnType=StructType | return pDF => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8a15f-81ad-467f-906c-706b68e19837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175304f0-b76d-4301-b21c-2c4b4298c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_tensor2.schema)\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e4086-04c9-47a4-b470-2a1abfea15ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = df_tensor2.columns\n",
    "preds = df_tensor2.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e30d7a-9143-46ca-bc0f-1f3c1a158da8",
   "metadata": {},
   "source": [
    "### pDF -> pDF | returnType=ArrayType(DoubleType()) | return pS => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa8b0f-b9c4-4565-9efa-e0b204b8827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c9d84-87d4-4efd-bfaf-3ec3ca6760ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch['t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9ac9e-3bc7-4668-befa-7f7507afe07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_tensor2.columns\n",
    "preds = df_tensor2.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21161bb-277f-4cb8-86b4-da54d0adba09",
   "metadata": {
    "tags": []
   },
   "source": [
    "### pDF -> pS | returnType=StructType | return pDF => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2bbf9-55e1-42eb-bcbd-cc5c4e3be058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd3d73-3e09-4803-b3cd-4c4bebf54f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_tensor2.schema)\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d4bfa-44ef-40bd-a2cb-c559e2123a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_tensor2.columns\n",
    "preds = df_tensor2.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ef869-d875-4664-8d79-98200ffad5c0",
   "metadata": {},
   "source": [
    "### pDF -> pS | returnType=ArrayType(DoubleType()) | return pS => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b651bd-977c-47d9-ae24-5e50fd40aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a0187-e1bb-4eaa-adcd-2e38fe4d2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch['t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c65670-b50e-44db-8a05-4da71ced8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_tensor2.columns\n",
    "preds = df_tensor2.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754200f4-00c7-433e-91dc-fc78c012d513",
   "metadata": {},
   "source": [
    "### Union[pDF, pS] -> Union[pDF, pS] | returnType=StructType() | return pS => FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd79653-af9b-402c-8593-66fabf8a9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3089081-0ea4-4b18-8560-beb0d946e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_tensor2.schema)\n",
    "# def predict(inputs: Iterator[Union[pd.DataFrame, pd.Series]]) -> Iterator[Union[pd.DataFrame, pd.Series]]:\n",
    "# def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[Union[pd.DataFrame, pd.Series]]:\n",
    "# def predict(inputs: Iterator[pd.Series]) -> Iterator[Union[pd.DataFrame, pd.Series]]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f0f1f-8f2f-45f5-9f1a-3275ccafb57e",
   "metadata": {},
   "source": [
    "## Test scalar columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0d3ca-69dc-4e2d-bbcc-ed1d37e831f2",
   "metadata": {},
   "source": [
    "### pDF -> pDF | returnType=StructType | return pDF => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc79165-ce6b-489a-a56c-71169e2290be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1510180-016c-4a56-9427-25b437ad3a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_scalar.schema)\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722dee2-ecc9-4f5a-8e3c-7dba759a7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_scalar.columns\n",
    "preds = df_scalar.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ad717-87a4-44a8-87c8-b01302143c29",
   "metadata": {},
   "source": [
    "### pDF -> pDF | returnType=DoubleType() | return pS => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f043b8-9755-4fe7-bb42-18eae5452d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916afdf-8f38-4fd0-8dfa-3f898174a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=DoubleType())\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d655206-ca3e-4a5c-a337-38f90213d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_scalar.columns\n",
    "preds = df_scalar.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc1abd-933e-4571-a735-c26ccf6e8d19",
   "metadata": {},
   "source": [
    "### pDF -> pS | returnType=StructType | return pDF => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da96e84-94c8-4c99-8831-544e855c7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7a314-f511-42f0-aff5-cc5a894b6f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_scalar.schema)\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babeda32-112d-4d6d-89fb-8f7ad24bc4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_scalar.columns\n",
    "preds = df_scalar.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e69ae2-0802-40d5-8d8c-918b8c1b6457",
   "metadata": {},
   "source": [
    "### pDF -> pS | returnType=DoubleType() | return pS => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead95954-f7a3-48e7-bd44-15d3b2a3d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7564de-9a76-474a-9666-6b7d875b17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=DoubleType())\n",
    "def predict(inputs: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d0f75-25d9-418e-9487-d7a8befc890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_scalar.columns\n",
    "preds = df_scalar.withColumn(\"preds\", predict(struct(*columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa8769-8a26-4cdd-9728-c304874c1b2f",
   "metadata": {},
   "source": [
    "### Union[pDF, pS] -> Union[pDF, pS] | returnType=StructType) | return pS => FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dcc5c-b4c1-49e2-889f-87ab6037442d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=df_scalar.schema)\n",
    "def predict(inputs: Iterator[Union[pd.DataFrame, pd.Series]]) -> Iterator[Union[pd.DataFrame, pd.Series]]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717ffa6-110c-4881-8c23-e69b16974609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341934d-625f-4f71-85bf-350a290275f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "from pyspark.sql.pandas.typehints import infer_eval_type\n",
    "from typing import get_type_hints, Any, Callable, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b8f77-a22b-4377-bae0-79a99eb92386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iterator_annotation(\n",
    "    annotation: Any, parameter_check_func: Optional[Callable[[Any], bool]] = None\n",
    ") -> bool:\n",
    "    name = getattr(annotation, \"_name\", getattr(annotation, \"__name__\", None))\n",
    "    return name == \"Iterator\" and (\n",
    "        parameter_check_func is None or all(map(parameter_check_func, annotation.__args__))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357203ee-f2ad-47c8-bed1-b0f0dc14caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_union_annotation(\n",
    "    annotation: Any, parameter_check_func: Optional[Callable[[Any], bool]] = None\n",
    ") -> bool:\n",
    "    import typing\n",
    "\n",
    "    # Note that we cannot rely on '__origin__' in other type hints as it has changed from version\n",
    "    # to version. For example, it's abc.Iterator in Python 3.7 but typing.Iterator in Python 3.6.\n",
    "    origin = getattr(annotation, \"__origin__\", None)\n",
    "    return origin == typing.Union and (\n",
    "        parameter_check_func is None or all(map(parameter_check_func, annotation.__args__))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33785c-39a2-4495-842b-08885d6b5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tuple_annotation(\n",
    "    annotation: Any, parameter_check_func: Optional[Callable[[Any], bool]] = None\n",
    ") -> bool:\n",
    "    # Tuple has _name but other types have __name__\n",
    "    # Check if the name is Tuple first. After that, check the generic types.\n",
    "    name = getattr(annotation, \"_name\", getattr(annotation, \"__name__\", None))\n",
    "    return name == \"Tuple\" and (\n",
    "        parameter_check_func is None or all(map(parameter_check_func, annotation.__args__))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab214bb-8e14-4b63-b3a7-294f86c75c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs: Iterator[Union[pd.Series, pd.DataFrame]]) -> Iterator[pd.DataFrame]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd7ca3-afee-4546-a0b4-c05f786d4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = signature(predict)\n",
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f075a-38b0-4475-b0e9-5da48a337a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_hints = get_type_hints(predict)\n",
    "type_hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4066b-1ea0-49e6-9591-a29ef46febee",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = {}\n",
    "for param in sig.parameters.values():\n",
    "    if param.annotation is not param.empty:\n",
    "        annotations[param.name] = type_hints.get(param.name, param.annotation)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce3e70-3a83-418a-b062-19e683994850",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sig = [\n",
    "    annotations[parameter] for parameter in sig.parameters if parameter in annotations]\n",
    "parameters_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe5863-75a4-40ac-9ddd-fb84230cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_annotation = type_hints.get(\"return\", sig.return_annotation)\n",
    "return_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dda63-60ed-4bda-b852-9618e4a52826",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_series_or_frame = all(\n",
    "    a == pd.Series\n",
    "    or a == pd.DataFrame  # Series\n",
    "    or check_union_annotation(  # DataFrame  # Union[DataFrame, Series]\n",
    "        a, parameter_check_func=lambda na: na == pd.Series or na == pd.DataFrame\n",
    "    )\n",
    "    for a in parameters_sig\n",
    ") and (return_annotation == pd.Series or return_annotation == pd.DataFrame)\n",
    "is_series_or_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e74b04-8bcb-423b-82ec-6f50ed11321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_iterator_tuple_series_or_frame = (\n",
    "    len(parameters_sig) == 1\n",
    "    and check_iterator_annotation(  # Iterator\n",
    "        parameters_sig[0],\n",
    "        parameter_check_func=lambda a: check_tuple_annotation(  # Tuple\n",
    "            a,\n",
    "            parameter_check_func=lambda ta: (\n",
    "                ta == Ellipsis\n",
    "                or ta == pd.Series  # ...\n",
    "                or ta == pd.DataFrame  # Series\n",
    "                or check_union_annotation(  # DataFrame  # Union[DataFrame, Series]\n",
    "                    ta, parameter_check_func=lambda na: (na == pd.Series or na == pd.DataFrame)\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    and check_iterator_annotation(\n",
    "        return_annotation, parameter_check_func=lambda a: a == pd.DataFrame or a == pd.Series\n",
    "    )\n",
    ")\n",
    "is_iterator_tuple_series_or_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a17191-d9f6-4de9-b6dd-5ec13e2d8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_iterator_series_or_frame = (\n",
    "    len(parameters_sig) == 1\n",
    "    and check_iterator_annotation(\n",
    "        parameters_sig[0],\n",
    "        parameter_check_func=lambda a: (\n",
    "            a == pd.Series\n",
    "            or a == pd.DataFrame  # Series\n",
    "            or check_union_annotation(  # DataFrame  # Union[DataFrame, Series]\n",
    "                a, parameter_check_func=lambda ua: ua == pd.Series or ua == pd.DataFrame\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    and check_iterator_annotation(\n",
    "        return_annotation, parameter_check_func=lambda a: a == pd.DataFrame or a == pd.Series\n",
    "    )\n",
    ")\n",
    "is_iterator_series_or_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bac7e8-2c6d-4702-842d-b396f00f98d9",
   "metadata": {},
   "source": [
    "### pS -> pS | returnType=ArrayType(DoubleType()) | return pS => OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a724d0-91d4-431c-b370-23306cfbce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029256e6-8320-4dad-91cf-01e6db08493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93759c1e-83f8-4d17-a4ac-90a5d58c3957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "def predict(inputs: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for batch in inputs:\n",
    "        print(\"===== batch: {}\".format(type(batch)))\n",
    "        print(\"===== len(batch: {}\".format(len(batch)))\n",
    "        # print(\"===== batch.columns: {}\".format(batch.columns))\n",
    "        # print(\"===== batch.dtypes:\\n{}\".format(batch.dtypes))\n",
    "        print(\"===== batch[0]:\\n{}\".format(batch[0]))\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f08667-baa0-48f3-b6a1-4006d4d8e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_scalar.columns\n",
    "preds = df_scalar.withColumn(\"preds\", predict(array(columns))).toPandas()\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c172c-6462-4226-843e-59905c5e3941",
   "metadata": {},
   "source": [
    "## Test caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae1db9-8ab6-4985-8d7c-4684da85f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pyspark.ml.functions import batch_infer_udf\n",
    "from pyspark.sql.functions import struct, pandas_udf\n",
    "from pyspark.sql.types import *\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129f2b5-c3da-4cf4-8b22-fdf9d4a158e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "\n",
    "# 4 scalar columns\n",
    "pdf = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\", \"d\"])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7dc28-3246-4ead-b76b-442a792f9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    # emulate loading a model, this should only be invoked once (per worker process)\n",
    "    fake_output = np.random.random()\n",
    "\n",
    "    def predict(inputs):\n",
    "        return [fake_output for i in inputs]\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221a5af-5060-46ee-adc3-d25b457023bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = batch_infer_udf(predict_batch_fn, return_type=DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd887b1-bbca-4a7f-ad3e-fa1e458d41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# results should be the same\n",
    "df1 = df.withColumn(\"preds\", identity(struct(\"a\"))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952c941-125e-442b-8c22-d82f6e0957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = df.withColumn(\"preds\", identity(struct(\"a\"))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aa60a-d1f7-4234-b21e-84c37e1155cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = df.withColumn(\"preds\", identity(struct(\"a\"))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6e8f2-12fc-4069-bb1d-5fba62e4c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = df.withColumn(\"preds\", identity(struct(\"a\"))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79589fbe-ac5a-40c5-bf07-c0341c1818dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = df.withColumn(\"preds\", identity(struct(\"a\"))).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc851cbd-bfe4-4d62-bb24-389610ebb7ff",
   "metadata": {},
   "source": [
    "## Test executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb93652-3adf-419e-bd05-a0695932afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pyspark.ml.functions import batch_infer_udf\n",
    "from pyspark.sql.functions import struct, pandas_udf\n",
    "from pyspark.sql.types import *\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d3b4f-726c-4d82-855c-afaf5670a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4)\n",
    "\n",
    "# 4 scalar columns\n",
    "pdf = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\", \"d\"])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe89306-d7fa-4bb7-8657-3b34212b6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfn(it):\n",
    "    import tensorflow as tf\n",
    "    print(\">>>> {}\".format(tf.__version__))\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    # Create some tensors\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52acb13-205b-478c-abb9-8283b99475c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.foreachPartition(myfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9112497-f078-4eeb-b220-a052a326b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=FloatType())\n",
    "def myudf(it: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for part in it:\n",
    "        import tensorflow as tf\n",
    "        print(tf.__version__)\n",
    "        yield part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d53a7b-02df-4820-b5b1-72258b2f429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = df.withColumn(\"preds\", struct(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f40c5b-36fe-472e-9ec6-9b49cc2785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67dc70-b052-4b39-a3a3-3cdb49ce6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== df1:\\n{}\".format(df1))\n",
    "print(\"==== df2:\\n{}\".format(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0fe5b-4936-4dbe-911b-6b22838d0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "df.withColumn(\"partition_id\", spark_partition_id()).withColumn(\"preds\", identity(struct(\"a\"))).groupBy(\"partition_id\", \"preds\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c3115-3e7f-4ca3-b26a-7b053812c09e",
   "metadata": {},
   "source": [
    "## Test zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7d410-8151-40eb-a82a-333f04204292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a992a7-1771-41d3-8068-2cb86d03bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.Series([0,1,2,3,4,5,6,7,8])\n",
    "bar = pd.Series(['a','b','c','d','e','f','g','h','i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe40800-65b0-4ff2-87ec-22d0ad3c9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (foo, bar)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887736f7-9733-4a19-ae7a-71a93d33cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, batch_size):\n",
    "    acc = []\n",
    "    for i, x in enumerate(zip(*iterable)):\n",
    "        acc += x\n",
    "        if i % batch_size == 0:\n",
    "            yield acc\n",
    "            acc = []\n",
    "    yield acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f4821-2209-4dcd-9361-794aa7ec48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "def batch(iterable, batch_size):\n",
    "    for x in more_itertools.chunked(zip(*iterable), batch_size):\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9ecfb-0859-4f4d-b618-33e694c8589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in batch(test, 2):\n",
    "    print(x)\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39599309-4dfe-402f-8957-feceeb03422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.concat(test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d00ab4-ed45-492c-9481-18c9526cac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c0ab7-e335-4179-ab73-057e376291c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [24, 24, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a3ac1-5f6c-491f-aa81-2a097b448e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = [-1] + shape\n",
    "batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c173413-eb30-4604-adae-4eacdcb755d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
