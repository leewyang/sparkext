{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04d288a-f4bf-4256-8ef8-fbc3ab92f24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Based on: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930360-c5ce-49ef-a69a-da88fa69a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02ba0a-8384-42b5-917c-53889b4a6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee64cf-a44a-4aff-82db-c64ee3a8b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644e508-5e4c-4cdd-9ed1-9235887d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X.astype(np.float32))\n",
    "            self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b55c3-dc7b-4831-9943-83efd48091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HousingDataset(X, y)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868f39d-4695-4110-91d2-6f7a09d73b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a441b60-dca4-44d2-bc1c-aa7336d704bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cff2b4-9d23-4d2b-808a-a5edb8eda135",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2db3f9-5db8-4b42-89ad-e77f23c4c1fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5):  # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace480ba-9316-49a4-9763-dc0f61f66989",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950a3ed-ffe1-477f-a84f-f71c85dbf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp, \"housing_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fedb5d-c59e-4b0b-ba91-3dd15df1f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted = torch.jit.script(mlp)\n",
    "scripted.save(\"housing_model.ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101c0fe-65f1-411e-9192-e8a6b585ba0d",
   "metadata": {},
   "source": [
    "### Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411b00f-88d2-40f5-b716-a26733c968ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mlp = torch.load(\"housing_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226f449-2931-4492-9003-503cdc61f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46af47e-db7e-42ee-9bd3-6e7d93850be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mlp(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae2c0f-1da5-45a4-bf32-ed8b562d7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e317f-c9bd-4f76-9463-7af2935d401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.load(\"housing_model.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8ec8-644e-4888-bfa0-b79425ece7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scripted_mlp(testX).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae694c-0127-4a61-8630-06004866cd14",
   "metadata": {},
   "source": [
    "### Columns as separate input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e11813-cf75-448e-a46e-f210cc7f52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from inspect import signature\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7da567-65df-4895-a867-0be05de27ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3340e01-b3bc-4cce-bb21-890517e1bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2ecd8-34e2-4ed5-8bd6-c9d56c951eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset2(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X.astype(np.float32))\n",
    "            self.y = torch.from_numpy(y.astype(np.float32))\n",
    "            \n",
    "            # Split dataset into separate variables\n",
    "            self.MedInc = self.X[:,0]\n",
    "            self.HouseAge = self.X[:,1]\n",
    "            self.AveRooms = self.X[:,2]\n",
    "            self.AveBedrms = self.X[:,3]\n",
    "            self.Population = self.X[:,4]\n",
    "            self.AveOccup = self.X[:,5]\n",
    "            self.Latitude = self.X[:,6]\n",
    "            self.Longitude = self.X[:,7]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MedInc)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Note: also returning combined X for ease of use later\n",
    "        return self.MedInc[i], self.HouseAge[i], self.AveRooms[i], self.AveBedrms[i], self.Population[i], self.AveOccup[i], self.Latitude[i], self.Longitude[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f4640-e190-413e-8e01-d67492408f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = HousingDataset2(housing.data, housing.target)\n",
    "trainloader2 = torch.utils.data.DataLoader(dataset2, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2179934-6ae0-4d58-ae2c-d82f90d48074",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(trainloader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3383b-fb1c-4daf-9844-88df7abf799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inc, age, rms, bdrms, pop, occup, lat, lon):       \n",
    "        combined = torch.column_stack((inc, age, rms, bdrms, pop, occup, lat, lon))\n",
    "        return self.layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9de54-a8da-46da-ba89-177a75227420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp2 = MLP2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631116aa-e496-4125-9970-14aeb816c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d0581-5911-4f21-b0c8-b94523f66dd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5):  # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader2, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        a,b,c,d,e,f,g,h,targets = data\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp2(a,b,c,d,e,f,g,h)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029a35d-8fbd-4a11-b3b0-55bcc0a072dd",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca720ac4-8b4e-489b-844f-d54dd0659755",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp2, \"housing_model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcced78-62ee-45fa-b334-6f73a2b21d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted = torch.jit.script(mlp2)\n",
    "scripted.save(\"housing_model2.ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb33c2-2e3f-487c-8b12-c4e8f13a67a2",
   "metadata": {},
   "source": [
    "### Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b12f2-9981-477b-8c21-a652a1736fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,targets = next(iter(trainloader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2fa69-9a8a-4409-8652-23c547536e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mlp2 = torch.load(\"housing_model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a21d52-ea98-4c74-98e8-1e088cdfa742",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mlp2(a,b,c,d,e,f,g,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d29fb0-5923-4684-8aa6-62618e8f1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signature(loaded_mlp2.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165772b2-8277-4b6e-a178-c99ea2a031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp2 = torch.jit.load(\"housing_model2.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d927e-fa6d-419d-b570-8ca9b0756812",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp2(a,b,c,d,e,f,g,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13631d1f-2c71-4bee-afcb-bd3b55ec87c5",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9937e-2c70-4d67-b95f-4d9d5ab17c12",
   "metadata": {},
   "source": [
    "### Convert dataset to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35da14-61a3-4e7b-9d4f-086bf5e931b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95148019-ea95-40e5-a529-fcdb9a06f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(housing.data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d957c-6747-4408-aac8-45305afbfe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(X, columns=housing.feature_names)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba338cd-76d2-46bd-baf5-7d18a339a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pdf.to_dict('series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b5036-d2ed-4edf-975f-66127862343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32ea98-a7f1-4011-a067-700377f1717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388cce9-6469-4f5a-898a-1a0b74eec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Spark is somehow auto-converting Pandas float32 to DoubleType(), so forcing FloatType()\n",
    "schema = StructType([\n",
    "StructField(\"MedInc\",FloatType(),True),\n",
    "StructField(\"HouseAge\",FloatType(),True),\n",
    "StructField(\"AveRooms\",FloatType(),True),\n",
    "StructField(\"AveBedrms\",FloatType(),True),\n",
    "StructField(\"Population\",FloatType(),True),\n",
    "StructField(\"AveOccup\",FloatType(),True),\n",
    "StructField(\"Latitude\",FloatType(),True),\n",
    "StructField(\"Longitude\",FloatType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(pdf, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33d367-fbf9-4918-b755-5447125547c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b144f-07ce-45b3-ab4f-dc26f205e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9932f-c21f-4eda-954e-26c38925ff84",
   "metadata": {},
   "source": [
    "### Save DataFrame as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bff7a-b687-4184-b3fa-b5f5b46ef5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261029ac-aa19-47cc-a423-ada08364f6a2",
   "metadata": {},
   "source": [
    "## Inference using Spark ML Model\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb1ffe-43c5-4aad-b468-ef515567f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparkext\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebd1a8-bd63-454c-a3b7-32e88cbe76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e63288-8c4e-45ea-b269-56cfae5b1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bea6ad-419c-4a66-9054-b5c45e4d114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inc, age, rms, bdrms, pop, occup, lat, lon):       \n",
    "        combined = torch.column_stack((inc, age, rms, bdrms, pop, occup, lat, lon))\n",
    "        return self.layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7ca36-172d-4748-a1d8-2b8e1f87d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = sparkext.torch.Model(\"housing_model2.pt\") \\\n",
    "                .setInputCols(columns) \\\n",
    "                .setOutputCol(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f12e7d-9d24-4873-b4f5-e1258d51197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = my_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ba1f0-4e6f-43ec-9b5f-6364a91a34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b07653-243b-48c3-a20b-e230002e132c",
   "metadata": {},
   "source": [
    "## Inference using Spark DL UDF\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5562968-a00c-43c6-b94f-87035dce0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from sparkext.torch import model_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27177f2-ed95-439c-85af-c58ed1e29aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375a6b2-8380-4de7-abc9-dc54dba40801",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a4f0d-0aa5-4f6d-b818-96b9bd5b20cf",
   "metadata": {},
   "source": [
    "### Using Saved Model\n",
    "\n",
    "Since the model is pickled, the model class must be defined before loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24308ad8-bad8-478b-aee6-a21b1c01ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inc, age, rms, bdrms, pop, occup, lat, lon):       \n",
    "        combined = torch.column_stack((inc, age, rms, bdrms, pop, occup, lat, lon))\n",
    "        return self.layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf4542-da40-4c4d-9d3a-ba11044aa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = model_udf(\"housing_model2.pt\", input_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d9207-2a13-4c6f-a91b-4003893694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = df.withColumn(\"preds\", classify(*columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0e808-6bb2-4a0c-ac52-02e2d0d464da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3472355-cc23-4016-998a-acf70b195710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions.show(truncate=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bb9ee-27fd-4604-89f8-6b438af0b984",
   "metadata": {},
   "source": [
    "## Inference using Spark DL API\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986d1a97-ea84-4707-b94a-78498780c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e40c266-24de-454d-a776-f3716ba50e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac802fb6-f159-4776-b55d-b9c421e8c57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8de001-e791-4a91-bd6f-c80bdf1c4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+\n",
      "|      MedInc|   HouseAge|    AveRooms|   AveBedrms| Population|     AveOccup|  Latitude| Longitude|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+\n",
      "|  0.85111564|  0.5053942|  0.29677927| -0.21630338| -0.3077235| 0.0066712764|-0.8576533| 0.7934686|\n",
      "|  0.12676695|  0.5053942| -0.21044567| -0.21279162| 0.20621344|  0.037127122|-0.8623344| 0.7934686|\n",
      "|  0.22788419| 0.26701993|-0.021574577|-0.046525124| 0.06845716|  0.064333126|-0.8576533| 0.7934686|\n",
      "|  0.83821946|  0.5053942|  0.46901828| -0.10617764| 0.16647606|  0.047397293|-0.8576533|  0.798461|\n",
      "| -0.09778573| 0.34647804| 0.040665437|-0.025475439| 0.24595083| -0.018044483|-0.8623344|  0.798461|\n",
      "|-0.114524566| -0.8453931|  0.08634951| -0.06806936|   0.348385|   -0.0268871|-0.8623344| 0.7834877|\n",
      "| -0.17863737| -1.0837674|  -0.3686518| -0.09916091|  0.7722505| -0.061561517|-0.8670172|0.78848004|\n",
      "| -0.30791578| -0.6070189|  -0.5994764| -0.12985049|  0.4084326| -0.029499885|-0.8623344|0.78848004|\n",
      "| 0.057443086| 0.18756187| -0.16621615|  -0.2677161| 0.42609367|  0.030650737|-0.8670172| 0.7934686|\n",
      "|  0.16377138| 0.42593613| 0.051558375| -0.35677052|-0.12051622| -0.014863143|-0.8623344|  0.798461|\n",
      "|   -0.403085|-0.76593506| -0.31700408|-0.064912155|  1.8795991| -0.080845445|-0.8670172|0.78848004|\n",
      "| 0.040651668|  0.5053942|-0.026731854| -0.16805135| 0.60447043|   0.05044324|-0.8670172|  0.798461|\n",
      "|  -0.3195487|-0.52756083| -0.32496482|-0.087023385|  1.3029655|  0.007953726|-0.8670172| 0.7934686|\n",
      "|  -0.2045352| 0.34647804| -0.12221943| -0.02406374|-0.27063525| -0.017252631|-0.8576533| 0.8034534|\n",
      "|  0.74268186|  0.5848523|   0.2167138| -0.19875023|-0.14524171| 0.0063703205|-0.8623344| 0.8034534|\n",
      "|     0.72289|  0.5053942| 0.024239399| -0.24278893| -0.5426156| -0.008573002|-0.8576533| 0.8034534|\n",
      "|  0.29178667| 0.34647804|  0.27256614| -0.16583116|-0.21235375|-8.3186955E-4|-0.8576533| 0.8034534|\n",
      "|    0.959497| 0.42593613|  0.38448015| -0.18836851|-0.16996719|-0.0025236963|-0.8623344|0.80844575|\n",
      "| 0.043651957|-0.20972852| -0.17829457|-0.055649087|  0.6698163|  -0.04076551|-0.8670172|0.80844575|\n",
      "|  -0.9490442|0.028645715| -0.67401487|  -0.1208191|-0.22559954| -0.061937403|-0.8623344| 0.8034534|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650dda22-31b7-419b-96d4-387a036f3b07",
   "metadata": {},
   "source": [
    "### Using TorchScript Model (single input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f45f5d-c941-4197-a274-1eec2af3fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    \n",
    "    scripted_mlp = torch.jit.load(\"/home/leey/devpub/leewyang/sparkext/examples/pytorch/housing_model.ts\")\n",
    "    scripted_mlp.to(device)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "        outputs = scripted_mlp(torch_inputs) # .flatten()\n",
    "        return outputs.detach().numpy()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220a00a4-e842-4f5d-a4b3-7693d09e2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=FloatType(),\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3bf287-8ffc-4456-8772-e97c418d6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 9.12 ms, total: 148 ms\n",
      "Wall time: 2.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", classify(struct(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764a40d8-25f7-425c-ba03-fe8c45f4b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|      MedInc|   HouseAge|    AveRooms|   AveBedrms| Population|     AveOccup|  Latitude| Longitude|    preds|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|  0.85111564|  0.5053942|  0.29677927| -0.21630338| -0.3077235| 0.0066712764|-0.8576533| 0.7934686|2.5334318|\n",
      "|  0.12676695|  0.5053942| -0.21044567| -0.21279162| 0.20621344|  0.037127122|-0.8623344| 0.7934686|1.9301689|\n",
      "|  0.22788419| 0.26701993|-0.021574577|-0.046525124| 0.06845716|  0.064333126|-0.8576533| 0.7934686|1.9071498|\n",
      "|  0.83821946|  0.5053942|  0.46901828| -0.10617764| 0.16647606|  0.047397293|-0.8576533|  0.798461|2.4639113|\n",
      "| -0.09778573| 0.34647804| 0.040665437|-0.025475439| 0.24595083| -0.018044483|-0.8623344|  0.798461|1.8513522|\n",
      "|-0.114524566| -0.8453931|  0.08634951| -0.06806936|   0.348385|   -0.0268871|-0.8623344| 0.7834877|1.7754748|\n",
      "| -0.17863737| -1.0837674|  -0.3686518| -0.09916091|  0.7722505| -0.061561517|-0.8670172|0.78848004|1.9242549|\n",
      "| -0.30791578| -0.6070189|  -0.5994764| -0.12985049|  0.4084326| -0.029499885|-0.8623344|0.78848004|1.9057091|\n",
      "| 0.057443086| 0.18756187| -0.16621615|  -0.2677161| 0.42609367|  0.030650737|-0.8670172| 0.7934686| 1.845237|\n",
      "|  0.16377138| 0.42593613| 0.051558375| -0.35677052|-0.12051622| -0.014863143|-0.8623344|  0.798461|1.8738399|\n",
      "|   -0.403085|-0.76593506| -0.31700408|-0.064912155|  1.8795991| -0.080845445|-0.8670172|0.78848004|1.9474475|\n",
      "| 0.040651668|  0.5053942|-0.026731854| -0.16805135| 0.60447043|   0.05044324|-0.8670172|  0.798461|1.8319652|\n",
      "|  -0.3195487|-0.52756083| -0.32496482|-0.087023385|  1.3029655|  0.007953726|-0.8670172| 0.7934686|1.8046335|\n",
      "|  -0.2045352| 0.34647804| -0.12221943| -0.02406374|-0.27063525| -0.017252631|-0.8576533| 0.8034534|1.7629256|\n",
      "|  0.74268186|  0.5848523|   0.2167138| -0.19875023|-0.14524171| 0.0063703205|-0.8623344| 0.8034534|2.4383569|\n",
      "|     0.72289|  0.5053942| 0.024239399| -0.24278893| -0.5426156| -0.008573002|-0.8576533| 0.8034534|2.4291956|\n",
      "|  0.29178667| 0.34647804|  0.27256614| -0.16583116|-0.21235375|-8.3186955E-4|-0.8576533| 0.8034534|1.9619908|\n",
      "|    0.959497| 0.42593613|  0.38448015| -0.18836851|-0.16996719|-0.0025236963|-0.8623344|0.80844575|2.6421642|\n",
      "| 0.043651957|-0.20972852| -0.17829457|-0.055649087|  0.6698163|  -0.04076551|-0.8670172|0.80844575| 2.089082|\n",
      "|  -0.9490442|0.028645715| -0.67401487|  -0.1208191|-0.22559954| -0.061937403|-0.8623344| 0.8034534|1.5327148|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6021192-4261-423d-8e79-171bd5d8ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:52:41 WARN TaskSetManager: Lost task 6.0 in stage 4.0 (TID 17) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n",
      "    raise ValueError(\n",
      "ValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:52)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/09/21 12:52:41 ERROR TaskSetManager: Task 6 in stage 4.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 8]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/dataframe.py:1137\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1137\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/utils.py:205\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    201\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", classify(*columns))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ded3e6-10ad-423e-bb94-96c769bdccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:52:42 WARN TaskSetManager: Lost task 5.0 in stage 4.0 (TID 16) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:52:43 WARN TaskSetManager: Lost task 6.0 in stage 5.0 (TID 25) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n",
      "    raise ValueError(\n",
      "ValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:52)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/09/21 12:52:43 ERROR TaskSetManager: Task 6 in stage 5.0 failed 1 times; aborting job\n",
      "22/09/21 12:52:43 WARN TaskSetManager: Lost task 5.0 in stage 5.0 (TID 24) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/21 12:52:43 WARN TaskSetManager: Lost task 7.0 in stage 5.0 (TID 26) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/21 12:52:43 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 20) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/21 12:52:43 WARN TaskSetManager: Lost task 4.0 in stage 5.0 (TID 23) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 7) / 8]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/dataframe.py:1137\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1137\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/utils.py:205\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    201\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", classify(*[col(c) for c in columns]))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773953dd-e645-4848-8f33-ed82f8242a43",
   "metadata": {},
   "source": [
    "### Using TorchScript Model (separate input variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a69a9d2-5c7f-4e71-bb65-ae51927ccacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7214e2ac-fd2c-473e-a9c7-a65488570b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee170b9-8ba6-4681-a10c-4cea71c1be15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "104b2378-e191-4560-9a2e-276b8dcf0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    scripted_mlp = torch.jit.load(\"/home/leey/devpub/leewyang/sparkext/examples/pytorch/housing_model2.ts\")\n",
    "    scripted_mlp.to(device)\n",
    "    \n",
    "    def predict(inc, age, rms, bdrms, pop, occ, lat, lon):\n",
    "        # inputs = [inc, age, rms, bdrms, pop, occ, lat, lon]\n",
    "        # torch_inputs = [torch.from_numpy(i).to(device) for i in inputs]\n",
    "        # outputs = scripted_mlp(*torch_inputs) #.flatten()\n",
    "        outputs = scripted_mlp(\n",
    "            torch.from_numpy(inc).to(device),\n",
    "            torch.from_numpy(age).to(device),\n",
    "            torch.from_numpy(rms).to(device),\n",
    "            torch.from_numpy(bdrms).to(device),\n",
    "            torch.from_numpy(pop).to(device),\n",
    "            torch.from_numpy(occ).to(device),\n",
    "            torch.from_numpy(lat).to(device),\n",
    "            torch.from_numpy(lon).to(device),\n",
    "        ) # .flatten()\n",
    "        return outputs.detach().numpy()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "020056dc-f8b0-483a-88eb-7e1ff2a0fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(predict_batch_fn,\n",
    "                             return_type=FloatType(),\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b73518e-04ec-49c7-bf1e-93520d94028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 7.28 ms, total: 159 ms\n",
      "Wall time: 1.46 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", classify(struct(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b56805-a211-43cb-878d-78957b08f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.9 ms, sys: 0 ns, total: 54.9 ms\n",
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", classify(*columns))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97c096d-6791-45cb-95df-f287e57b4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 16.4 ms, total: 153 ms\n",
      "Wall time: 444 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", classify(*[col(c) for c in columns]))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5032b474-db92-4f04-b732-8b9d418cf211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|      MedInc|   HouseAge|    AveRooms|   AveBedrms| Population|     AveOccup|  Latitude| Longitude|    preds|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|  0.85111564|  0.5053942|  0.29677927| -0.21630338| -0.3077235| 0.0066712764|-0.8576533| 0.7934686|2.5334318|\n",
      "|  0.12676695|  0.5053942| -0.21044567| -0.21279162| 0.20621344|  0.037127122|-0.8623344| 0.7934686|1.9301689|\n",
      "|  0.22788419| 0.26701993|-0.021574577|-0.046525124| 0.06845716|  0.064333126|-0.8576533| 0.7934686|1.9071498|\n",
      "|  0.83821946|  0.5053942|  0.46901828| -0.10617764| 0.16647606|  0.047397293|-0.8576533|  0.798461|2.4639113|\n",
      "| -0.09778573| 0.34647804| 0.040665437|-0.025475439| 0.24595083| -0.018044483|-0.8623344|  0.798461|1.8513522|\n",
      "|-0.114524566| -0.8453931|  0.08634951| -0.06806936|   0.348385|   -0.0268871|-0.8623344| 0.7834877|1.7754748|\n",
      "| -0.17863737| -1.0837674|  -0.3686518| -0.09916091|  0.7722505| -0.061561517|-0.8670172|0.78848004|1.9242549|\n",
      "| -0.30791578| -0.6070189|  -0.5994764| -0.12985049|  0.4084326| -0.029499885|-0.8623344|0.78848004|1.9057091|\n",
      "| 0.057443086| 0.18756187| -0.16621615|  -0.2677161| 0.42609367|  0.030650737|-0.8670172| 0.7934686| 1.845237|\n",
      "|  0.16377138| 0.42593613| 0.051558375| -0.35677052|-0.12051622| -0.014863143|-0.8623344|  0.798461|1.8738399|\n",
      "|   -0.403085|-0.76593506| -0.31700408|-0.064912155|  1.8795991| -0.080845445|-0.8670172|0.78848004|1.9474475|\n",
      "| 0.040651668|  0.5053942|-0.026731854| -0.16805135| 0.60447043|   0.05044324|-0.8670172|  0.798461|1.8319652|\n",
      "|  -0.3195487|-0.52756083| -0.32496482|-0.087023385|  1.3029655|  0.007953726|-0.8670172| 0.7934686|1.8046335|\n",
      "|  -0.2045352| 0.34647804| -0.12221943| -0.02406374|-0.27063525| -0.017252631|-0.8576533| 0.8034534|1.7629256|\n",
      "|  0.74268186|  0.5848523|   0.2167138| -0.19875023|-0.14524171| 0.0063703205|-0.8623344| 0.8034534|2.4383569|\n",
      "|     0.72289|  0.5053942| 0.024239399| -0.24278893| -0.5426156| -0.008573002|-0.8576533| 0.8034534|2.4291956|\n",
      "|  0.29178667| 0.34647804|  0.27256614| -0.16583116|-0.21235375|-8.3186955E-4|-0.8576533| 0.8034534|1.9619908|\n",
      "|    0.959497| 0.42593613|  0.38448015| -0.18836851|-0.16996719|-0.0025236963|-0.8623344|0.80844575|2.6421642|\n",
      "| 0.043651957|-0.20972852| -0.17829457|-0.055649087|  0.6698163|  -0.04076551|-0.8670172|0.80844575| 2.089082|\n",
      "|  -0.9490442|0.028645715| -0.67401487|  -0.1208191|-0.22559954| -0.061937403|-0.8623344| 0.8034534|1.5327148|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf50b1-b79d-4ae1-b6f9-5378387da72c",
   "metadata": {},
   "source": [
    "### Using Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b0b5a19-a0fa-4bfd-9581-d135988c7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eae04bc-75ca-421a-87c8-ac507ce1f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"california_housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b350bd8e-9b8f-4511-9ddf-76d917b21b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f8022-d9a4-4f60-bfa0-c37241d24292",
   "metadata": {},
   "source": [
    "#### Start Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6fd1612-de6a-461c-a2ad-1a3fcd277d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_executors = 1\n",
    "\n",
    "nodeRDD = sc.parallelize(list(range(num_executors)), num_executors)\n",
    "\n",
    "def start_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    if containers:\n",
    "        print(\">>>> containers: {}\".format([c.short_id for c in containers]))\n",
    "    else:\n",
    "        container=client.containers.run(\n",
    "            \"nvcr.io/nvidia/tritonserver:22.07-py3\", \"tritonserver --model-repository=/models\",\n",
    "            detach=True,\n",
    "            device_requests=[docker.types.DeviceRequest(device_ids=[\"0\"], capabilities=[['gpu']])],\n",
    "            name=\"spark-triton\",\n",
    "            network_mode=\"host\",\n",
    "            remove=True,\n",
    "            shm_size=\"64M\",\n",
    "            volumes={\"/home/leey/devpub/leewyang/sparkext/examples/models\": {\"bind\": \"/models\", \"mode\": \"ro\"}}\n",
    "        )\n",
    "        print(\">>>> starting triton: {}\".format(container.short_id))\n",
    "\n",
    "        # wait for triton to be running\n",
    "        time.sleep(15)\n",
    "        client = grpcclient.InferenceServerClient(\"localhost:8001\")\n",
    "        ready = False\n",
    "        while not ready:\n",
    "            try:\n",
    "                ready = client.is_server_ready()\n",
    "            except Exception as e:\n",
    "                time.sleep(5)\n",
    "            \n",
    "    return [True]\n",
    "\n",
    "nodeRDD.mapPartitions(start_triton).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b343ec-688d-4e4d-985e-db72beaaf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool8),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3e64fda-117b-4810-a9a2-dd498239496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = predict_batch_udf(partial(triton_fn, triton_uri=\"localhost:8001\", model_name=\"housing_model\"),\n",
    "                             return_type=FloatType(),\n",
    "                             batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a24149a5-3adc-4089-8769-13cf1e44547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86 ms, sys: 10 ms, total: 96 ms\n",
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first pass caches model/fn\n",
    "predictions = df.withColumn(\"preds\", classify(struct(*columns)))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b79c62c8-e1e8-4467-8aef-8939c31833b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|      MedInc|   HouseAge|    AveRooms|   AveBedrms| Population|     AveOccup|  Latitude| Longitude|    preds|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "|  0.85111564|  0.5053942|  0.29677927| -0.21630338| -0.3077235| 0.0066712764|-0.8576533| 0.7934686| 2.533432|\n",
      "|  0.12676695|  0.5053942| -0.21044567| -0.21279162| 0.20621344|  0.037127122|-0.8623344| 0.7934686|1.9301689|\n",
      "|  0.22788419| 0.26701993|-0.021574577|-0.046525124| 0.06845716|  0.064333126|-0.8576533| 0.7934686|1.9071497|\n",
      "|  0.83821946|  0.5053942|  0.46901828| -0.10617764| 0.16647606|  0.047397293|-0.8576533|  0.798461|2.4639113|\n",
      "| -0.09778573| 0.34647804| 0.040665437|-0.025475439| 0.24595083| -0.018044483|-0.8623344|  0.798461| 1.851352|\n",
      "|-0.114524566| -0.8453931|  0.08634951| -0.06806936|   0.348385|   -0.0268871|-0.8623344| 0.7834877|1.7754749|\n",
      "| -0.17863737| -1.0837674|  -0.3686518| -0.09916091|  0.7722505| -0.061561517|-0.8670172|0.78848004|1.9242551|\n",
      "| -0.30791578| -0.6070189|  -0.5994764| -0.12985049|  0.4084326| -0.029499885|-0.8623344|0.78848004|1.9057091|\n",
      "| 0.057443086| 0.18756187| -0.16621615|  -0.2677161| 0.42609367|  0.030650737|-0.8670172| 0.7934686|1.8452368|\n",
      "|  0.16377138| 0.42593613| 0.051558375| -0.35677052|-0.12051622| -0.014863143|-0.8623344|  0.798461|1.8738399|\n",
      "|   -0.403085|-0.76593506| -0.31700408|-0.064912155|  1.8795991| -0.080845445|-0.8670172|0.78848004|1.9474474|\n",
      "| 0.040651668|  0.5053942|-0.026731854| -0.16805135| 0.60447043|   0.05044324|-0.8670172|  0.798461|1.8319653|\n",
      "|  -0.3195487|-0.52756083| -0.32496482|-0.087023385|  1.3029655|  0.007953726|-0.8670172| 0.7934686|1.8046334|\n",
      "|  -0.2045352| 0.34647804| -0.12221943| -0.02406374|-0.27063525| -0.017252631|-0.8576533| 0.8034534|1.7629259|\n",
      "|  0.74268186|  0.5848523|   0.2167138| -0.19875023|-0.14524171| 0.0063703205|-0.8623344| 0.8034534|2.4383569|\n",
      "|     0.72289|  0.5053942| 0.024239399| -0.24278893| -0.5426156| -0.008573002|-0.8576533| 0.8034534|2.4291954|\n",
      "|  0.29178667| 0.34647804|  0.27256614| -0.16583116|-0.21235375|-8.3186955E-4|-0.8576533| 0.8034534|1.9619908|\n",
      "|    0.959497| 0.42593613|  0.38448015| -0.18836851|-0.16996719|-0.0025236963|-0.8623344|0.80844575|2.6421642|\n",
      "| 0.043651957|-0.20972852| -0.17829457|-0.055649087|  0.6698163|  -0.04076551|-0.8670172|0.80844575| 2.089082|\n",
      "|  -0.9490442|0.028645715| -0.67401487|  -0.1208191|-0.22559954| -0.061937403|-0.8623344| 0.8034534|1.5327147|\n",
      "+------------+-----------+------------+------------+-----------+-------------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca6f3eaa-9569-45d0-88bf-9aa0757e1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:53:25 WARN TaskSetManager: Lost task 6.0 in stage 15.0 (TID 70) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n",
      "    raise ValueError(\n",
      "ValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:52)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/09/21 12:53:25 ERROR TaskSetManager: Task 6 in stage 15.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/dataframe.py:1137\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1137\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/utils.py:205\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    201\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", classify(*columns))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a18d76-7868-47a9-b27f-b57c6580299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:53:26 WARN TaskSetManager: Lost task 5.0 in stage 15.0 (TID 69) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/21 12:53:26 WARN TaskSetManager: Lost task 6.0 in stage 16.0 (TID 78) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n",
      "    raise ValueError(\n",
      "ValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:52)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:889)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/09/21 12:53:26 ERROR TaskSetManager: Task 6 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 8]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/dataframe.py:1137\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1137\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/sql/utils.py:205\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    201\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/leewyang/spark/python/pyspark/ml/functions.py\", line 303, in predict\n    raise ValueError(\nValueError: Multiple input columns found, but model expected a single input, use `struct` or `array` to combine columns into tensors.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = df.withColumn(\"preds\", classify(*[col(c) for c in columns]))\n",
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec23b0-eaf2-4b6a-aa38-7a09873ed6eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stop Triton Server on each executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15e9b3df-f3c9-46bb-bbeb-42496f7663de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/21 12:53:26 WARN TaskSetManager: Lost task 7.0 in stage 16.0 (TID 79) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_triton(it):\n",
    "    import docker\n",
    "    import time\n",
    "    \n",
    "    client=docker.from_env()\n",
    "    containers=client.containers.list(filters={\"name\": \"spark-triton\"})\n",
    "    print(\">>>> stopping containers: {}\".format([c.short_id for c in containers]))\n",
    "    if containers:\n",
    "        container=containers[0]\n",
    "        container.stop(timeout=120)\n",
    "\n",
    "    return [True]\n",
    "\n",
    "nodeRDD.mapPartitions(stop_triton).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0138a029-87c5-497f-ac5c-3eed0e11b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24147e7-5695-44a0-9961-b94bfba1cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
