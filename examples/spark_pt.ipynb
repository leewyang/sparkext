{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792a56d6-0ea6-4e8f-bef9-b578c0abec41",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark PyTorch Training\n",
    "\n",
    "Based on:\n",
    "- https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14e5d8-fd66-417c-bfef-5ca063dc6942",
   "metadata": {},
   "source": [
    "## Distributed Training on executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f6363-ffff-4a83-9a27-47a175581a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# On Windows platform, the torch.distributed package only\n",
    "# supports Gloo backend, FileStore and TcpStore.\n",
    "# For FileStore, set init_method parameter in init_process_group\n",
    "# to a local file. Example as follow:\n",
    "# init_method=\"file:///f:/libtmp/some_file\"\n",
    "# dist.init_process_group(\n",
    "#    \"gloo\",\n",
    "#    rank=rank,\n",
    "#    init_method=init_method,\n",
    "#    world_size=world_size)\n",
    "# For TcpStore, same way as on Linux.\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f019e2-3faf-463b-8482-bf0e55cd1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09dd19-f6ef-4905-b3f2-4e37263d4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8823e-8c33-496e-a425-f31274621feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbfae6-f274-4b48-94a9-afbccd3faa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(partition):\n",
    "    import json\n",
    "    import os\n",
    "    import random\n",
    "    import socket\n",
    "    import torch\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from pyspark import BarrierTaskContext\n",
    "    from datetime import datetime\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torchvision import datasets\n",
    "    from torchvision.transforms import ToTensor\n",
    "\n",
    "    # data\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=64)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "    # get list of participating nodes and my local address\n",
    "    context = BarrierTaskContext.get()\n",
    "    task_infos = context.getTaskInfos()\n",
    "    workers = [t.address.split(':')[0] for t in task_infos]\n",
    "    rank = context.partitionId()\n",
    "    world_size = len(workers)\n",
    "    \n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    \n",
    "    model = NeuralNetwork()\n",
    "    ddp_model = DDP(model)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce24e-0f64-4634-849f-6c0061436ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeRDD = sc.parallelize(range(2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0d47d-3123-44aa-95c0-2296eabe5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeRDD.barrier().mapPartitions(train_fn).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c4a30-2c88-426c-b8dd-02c7a48fdf15",
   "metadata": {},
   "source": [
    "## Refactored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739542d-7e83-4b3a-be87-38c9667e3a33",
   "metadata": {},
   "source": [
    "### Read from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f158179e-2ba9-4f5d-a6f4-76a3ac8d8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from pyspark import BarrierTaskContext\n",
    "\n",
    "class FrameworkPlugin(ABC):\n",
    "    @staticmethod\n",
    "    def setup(context: BarrierTaskContext):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def teardown():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5573a1-3240-4fd4-b52c-9c77c8172b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "class PyTorchPlugin(FrameworkPlugin):\n",
    "    @staticmethod\n",
    "    def setup(context: BarrierTaskContext):\n",
    "        import json\n",
    "        import socket\n",
    "\n",
    "        task_infos = context.getTaskInfos()\n",
    "        workers = [t.address.split(':')[0] for t in task_infos]\n",
    "        rank = context.partitionId()\n",
    "        world_size = len(workers)\n",
    "        my_addr = workers[rank]\n",
    "\n",
    "        # find available port for master using allGather as a proxy for broadcast\n",
    "        if rank == 0:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "                sock.bind(('', 0))\n",
    "                _, port = sock.getsockname()\n",
    "                master = \"{}:{}\".format(my_addr, port)\n",
    "                master_candidates = context.allGather(master)\n",
    "        else:\n",
    "            # all nodes must invoke allGather\n",
    "            master_candidates = context.allGather(\"\")\n",
    "\n",
    "        addr, port = master_candidates[0].split(':')\n",
    "        print(f\"Assigning master to: {addr}:{port} on rank {rank}\")\n",
    "        print(f\"Running basic DDP example on rank {rank}.\")\n",
    "        os.environ['MASTER_ADDR'] = addr\n",
    "        os.environ['MASTER_PORT'] = port\n",
    "        \n",
    "        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "        return rank, world_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def teardown():\n",
    "        dist.destroy_process_group()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2d45db-93fb-44e7-9594-9edd3615da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def distribute(framework_plugin):\n",
    "    def decorator_distribute(train_fn):\n",
    "        @functools.wraps(train_fn)\n",
    "        def _wrapper(df_iter):\n",
    "            from pyspark import BarrierTaskContext\n",
    "\n",
    "            # get list of participating nodes and my local address\n",
    "            context = BarrierTaskContext.get()\n",
    "            rank, world_size = framework_plugin.setup(context)    \n",
    "            result = train_fn(df_iter)\n",
    "            framework_plugin.teardown()\n",
    "            return result\n",
    "        return _wrapper\n",
    "    return decorator_distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d1f136-a9b1-43b8-b7ec-12143fd27748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8244414f-e333-45ad-9b7c-b56f7227f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"batch: {batch:>5}, loss: {loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232a3723-1caa-4c83-9a77-24a8b70e5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1a190e-5b23-4342-8291-5c46bd87c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@distribute(PyTorchPlugin)\n",
    "def train_fn(partition):\n",
    "    import json\n",
    "    import os\n",
    "    import random\n",
    "    import socket\n",
    "    import torch\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torchvision import datasets\n",
    "    from torchvision.transforms import ToTensor\n",
    "\n",
    "    # data\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"/home/leey/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"/home/leey/data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=64)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "    \n",
    "    model = NeuralNetwork()\n",
    "    ddp_model = DDP(model)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, ddp_model, loss_fn, optimizer)\n",
    "        test_loop(test_dataloader, ddp_model, loss_fn)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567a9b7f-97dc-441b-b363-9b38826e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeRDD = sc.parallelize(range(2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4772fcc-1462-4914-81b3-47388867b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodeRDD.barrier().mapPartitions(train_fn).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69169c47-2fbd-4b1b-ba8f-b2e35e638dc0",
   "metadata": {},
   "source": [
    "### Read from Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51053c43-02bf-432a-b4b8-89b8ef975c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e705c-adc3-4930-8952-8edfd9be32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/leey/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/leey/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43f821-76f5-46dc-86a8-3012506e0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = training_data.data.numpy()\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66f5e2-e18c-4862-85a4-fa30fdf8f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data.targets.numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a90d5f-adf3-4740-8cbd-1ecfcf665086",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf784 = pd.DataFrame(images.reshape(-1, 784) / 255.0)\n",
    "pdf784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5219b-f166-47cd-b319-9b25bc35b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = pd.DataFrame()\n",
    "pdf1['image'] = pdf784.values.tolist()\n",
    "pdf1['label'] = labels\n",
    "pdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894738e-9da6-4f43-90b6-40f978586e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType, IntegerType\n",
    "\n",
    "# force FloatType since Spark defaults to DoubleType\n",
    "schema = StructType([\n",
    "    StructField(\"image\",ArrayType(FloatType()), True),\n",
    "    StructField(\"label\",IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(pdf1, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db79a5-52d5-4f19-aabe-c04a620e88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"fashion_mnist_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea825697-ddf6-4084-8b92-19cf51af6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/home/leey/dev/nvsparkdl/examples/fashion_mnist_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3455d868-b38d-4bd5-99bf-ab593d46f580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|[0.0, 0.0, 0.0, 0...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    0|\n",
      "|[0.0, 0.0, 0.0, 0...|    8|\n",
      "|[0.0, 0.0, 0.0, 0...|    6|\n",
      "|[0.0, 0.0, 0.0039...|    0|\n",
      "|[0.0, 0.0, 0.0039...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    1|\n",
      "|[0.0, 0.0, 0.0, 0...|    1|\n",
      "|[0.0, 0.0, 0.0, 0...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    9|\n",
      "|[0.0, 0.0, 0.0, 0...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    8|\n",
      "|[0.0, 0.0, 0.0, 0...|    6|\n",
      "|[0.0, 0.0, 0.0, 0...|    7|\n",
      "|[0.0, 0.0, 0.0, 0...|    7|\n",
      "|[0.0, 0.0, 0.0, 0...|    5|\n",
      "|[0.0, 0.0, 0.0, 0...|    2|\n",
      "|[0.0, 0.0, 0.0, 0...|    6|\n",
      "|[0.0, 0.0, 0.0, 0...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07522f7a-90f4-4ef9-8885-4023d37451a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@distribute(PyTorchPlugin)\n",
    "def train_fn(partition):\n",
    "    import json\n",
    "    import os\n",
    "    import random\n",
    "    import socket\n",
    "    import torch\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torchvision import datasets\n",
    "    from torchvision.transforms import ToTensor\n",
    "\n",
    "    # Receive data from Spark\n",
    "    # for pdf in partition:\n",
    "    #     foo = pdf.to_numpy()\n",
    "    #     bar = [tuple(x) for x in foo]\n",
    "       \n",
    "    foo = [pdf.to_numpy() for pdf in partition]\n",
    "    baz = np.concatenate(foo)\n",
    "    bar = [tuple(x) for x in baz]\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(bar, batch_size=64)\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "    \n",
    "    model = NeuralNetwork()\n",
    "    ddp_model = DDP(model)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for t in range(5):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, ddp_model, loss_fn, optimizer)\n",
    "        # test_loop(test_dataloader, ddp_model, loss_fn)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f5382c-efb2-46b6-88c6-d441284ec597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_out = df \\\n",
    "    .repartition(2) \\\n",
    "    .mapInPandas(train_fn, schema=\"image array<int>, label float\") \\\n",
    "    .rdd \\\n",
    "    .barrier() \\\n",
    "    .mapPartitions(lambda x: x) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05702a77-4571-44aa-8b0d-251c551d8d57",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c116f-a97b-4278-a83f-be71880b1304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
