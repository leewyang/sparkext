{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd2accf-5877-4136-a243-7a33a13ce2b4",
   "metadata": {},
   "source": [
    "# Pyspark TensorFlow Inference\n",
    "\n",
    "## Text classification\n",
    "Based on: https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0f5df-502f-444e-b2ee-1122e1dea870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364ad5f-b269-45b5-ab8b-d8f34fb642b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229c1b6-3967-46b5-9ea8-68f4b42dd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    ")\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8038ae-8bc1-46bf-ae4c-6da08886c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faaa3f-3441-4361-b9eb-4317e8c2c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir, \"train\")\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc0cc-65d0-4e17-9ee8-222390df45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = os.path.join(train_dir, \"pos/1181_9.txt\")\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2277f58-78c8-4a12-bc98-5103e7c81a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, \"unsup\")\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83de92-ebb3-4170-b2bf-25265c6a6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c30568-daa8-4b2b-b30a-577c984a8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e863eb6-4bd7-4da0-b10d-d951b5ee52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593e2e5-df51-4fbf-b4be-c786e740ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fd61d-3926-4296-889a-b2a375a1b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb141709-fcc1-4cee-bc98-9c89aaba8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e80ea9-536a-4ebc-8b35-1eca73dbba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e5d81-7dae-4b08-b520-ca45501b9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f243f5-edd3-4e1c-bddc-abc1cc6673ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37e95c-515c-4edb-a1ee-fc47be5df4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9208a-39ac-4e6c-a603-61038cdf3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf90d4b-8dae-44b2-b32b-80cb0092c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a5aba-8a00-458f-be25-0aae9f55de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f4495-102d-4244-9b42-1ba9976a366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc95d22-935f-4091-b0ee-da95174eb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9059b93-7666-46db-bf15-517c4c205df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5959f-1bd8-48da-9815-8239599519b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656afe07-354f-4ff2-8e3e-d02bad6c5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d0f13-d0b8-4d78-9ddc-ede5ed402446",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7484c3-3cdf-46d5-b95d-80316f0e6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51178e-fe0b-40ca-9260-2190fb52d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0a42c-437e-41bb-99e7-d58cb8036a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939539b-a600-48b1-a55e-3f1087f4a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "\n",
    "export_model.predict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b40a59-8d3b-44ec-a4f7-92c5742a0c1c",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa1770-2bf9-436b-af32-16b64fc97490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22cc32-2708-4808-8e76-99024da87a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.save('text_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add8a92-bb90-415b-ae21-417a6b722a8b",
   "metadata": {},
   "source": [
    "### Inspect saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dda184-8288-4de2-95a9-41d0a9744b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756f01d-dcef-4132-a0e6-4231792356de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir text_model --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0461f74-fdd0-4f30-9f44-0be7ad00d9b0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf2c7f-5e86-4ff8-984e-dd0ed7a3ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register callables as custom objects before loading\n",
    "custom_objects = {\"vectorize_layer\": vectorize_layer, \"custom_standardization\": custom_standardization}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    new_model = tf.keras.models.load_model('text_model')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a4f7e-fa45-4d21-b103-fe3718bc0f10",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531680b2-42ef-4205-9a38-6995aee9f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ae387-1587-4175-b4b2-66586e4668f7",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc64ea70-4a41-40f8-b1a5-eee23e8953ac",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13502d31-dc06-4161-8cb4-f70e11e0be48",
   "metadata": {},
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58d292d3-2a73-450d-b0a0-74040cafc27c",
   "metadata": {},
   "source": [
    "# convert tf.data.Dataset of tf.Tensor -> list of numpy [string]\n",
    "np_data = [review[0].numpy().astype(str) for review in raw_test_ds]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79f9b1c4-57f9-4c2f-a7c4-8796f34bff35",
   "metadata": {},
   "source": [
    "len(np_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b98eb92a-185d-4d50-945a-3610120cd5e9",
   "metadata": {},
   "source": [
    "np_data[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5506c35a-8c6c-4949-9083-12fcebb6ea2b",
   "metadata": {},
   "source": [
    "type(np_data[index][0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9bd1890-1072-4eff-8cd1-2c2eac070ef3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "25ac04e4-6aa4-4a09-9745-cf431f5e4066",
   "metadata": {},
   "source": [
    "np_data[index][0][250:260]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7eb5dd7-5c20-49fb-aec0-8c63275fdb33",
   "metadata": {},
   "source": [
    "# convert numpy array to pandas dataframe\n",
    "pdf = pd.DataFrame(np_data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f2a1587-b292-4d43-a5fe-086dc8fce34b",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "raw",
   "id": "add3196f-0891-4f1e-a121-522564094846",
   "metadata": {},
   "source": [
    "# convert pandas dataframe to spark dataframe\n",
    "schema = StructType([StructField(\"reviews\",StringType(),True)])\n",
    "df = spark.createDataFrame(pdf, schema).repartition(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6a25c2c-c2c1-4149-a6df-b4ff3cfb0190",
   "metadata": {},
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bafa714a-172a-4a84-a0de-1076aa234253",
   "metadata": {},
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"imdb_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bd880-d631-40ed-838b-40e81b44bc54",
   "metadata": {},
   "source": [
    "### Check arrow memory configuration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "128b1b37-1a4e-4af6-a515-5e80741ade2a",
   "metadata": {},
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"512\")\n",
    "# This line will fail if the vectorized reader runs out of memory\n",
    "assert len(df.head()) > 0, \"`df` should not be empty\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c37f-de28-4b94-adb6-ea43b937afb4",
   "metadata": {},
   "source": [
    "## Inference using Spark ML Model\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea1c53-b0ac-48d4-9316-697149c410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import sparkext\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59cad5-25a5-47ec-a1d8-631b868bc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: using huggingface IMDB parquet for now, since conversion above isn't working\n",
    "\n",
    "df = spark.read.parquet(\"/Users/leey/dev/sparkext/examples/huggingface/imdb_test\").repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783aa84-98c1-4920-a501-1c6b8c993727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5b934-5bb5-4658-900e-9b970d7bfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(path: str):\n",
    "    import re\n",
    "    import string\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    def custom_standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "        return tf.strings.regex_replace(\n",
    "            stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "        )\n",
    "    \n",
    "    max_features = 10000\n",
    "    sequence_length = 250\n",
    "\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    \n",
    "    custom_objects = {\"vectorize_layer\": vectorize_layer, \"custom_standardization\": custom_standardization}\n",
    "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "        model = tf.keras.models.load_model(path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91517d9-db4e-4d36-8f23-dfd1632c4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: must use absolute path to model, since executors have their own current working directories\n",
    "model = sparkext.tensorflow.Model(\"/Users/leey/dev/sparkext/examples/tensorflow/text_model\", model_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bc2f6-8409-47c1-a002-9342bd5337cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c423b5e-b663-4d17-8cf5-774e116731ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write.mode(\"overwrite\").parquet(\"mnist_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7c104-d32c-4189-bdad-7ed463e39315",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f16a23-e89b-40b8-850f-2ebee3a14192",
   "metadata": {},
   "source": [
    "## Inference using Spark DL UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55396dc-cddb-479f-bb57-f1307980c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from sparkext.tensorflow import model_udf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7ca24-e052-4fa9-9e40-cbada9df58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path(\"../huggingface/imdb_test\").absolute().as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be145cad-56e0-48cd-bd8e-06792e066b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: using huggingface IMDB parquet for now, since conversion above isn't working\n",
    "df = spark.read.parquet(dataset).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3fbc-6fa2-407f-98a4-07cefc227661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a844a-136c-4729-9d65-0fbc7e306e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(path: str):\n",
    "    # since this function runs on the executor, any required imports should be added inside the function.\n",
    "    import re\n",
    "    import string\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    def custom_standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "        return tf.strings.regex_replace(\n",
    "            stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "        )\n",
    "    \n",
    "    max_features = 10000\n",
    "    sequence_length = 250\n",
    "\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    \n",
    "    custom_objects = {\"vectorize_layer\": vectorize_layer, \"custom_standardization\": custom_standardization}\n",
    "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "        model = tf.keras.models.load_model(path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcda3a-8825-46d9-8d42-7dd86b9ccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: must use absolute path to model, since executors have their own current working directories\n",
    "model_path = Path(\"text_model\").absolute().as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58cadf-f766-4e07-b1d7-1ab06bb00350",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = model_udf(model_path, model_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeca682-1ae7-48e5-b602-2cb91854eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"preds\", classify(col(\"lines\"))).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12713bc8-f072-4172-8928-03ac5c82b71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
