{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d9e485-1fcc-444a-9747-2da24efec7ee",
   "metadata": {},
   "source": [
    "# Pyspark TensorFlow Inference\n",
    "\n",
    "## Image classification\n",
    "Based on: https://www.tensorflow.org/tutorials/keras/save_and_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a639cf-4949-4c70-8cd4-7c49a17aa34f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60637a24-0b28-4d11-9082-14ae0a9c52b1",
   "metadata": {},
   "source": [
    "### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375dd090-86e6-4113-a634-a47e1e8b5acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset as numpy arrays\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0946cb-9ca1-4837-aedd-5e9b10207298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize\n",
    "train_images = train_images.astype(np.float32).reshape(-1, 784) / 255.0\n",
    "test_images = test_images.astype(np.float32).reshape(-1, 784) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded725b2-cd66-40f5-a0ad-aafc1a5e3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd77836-3fd3-4a77-9d01-e657d09aac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float32'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_images), train_images.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576add9-2474-42f9-85a6-3945a7bbd9cc",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363c8202-1cdb-48e6-b934-b55ae84eeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:58:37.086555: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287461f-d9c3-4530-955a-818807380a17",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92719ca1-0108-45b5-9821-e5c0b9fac4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2212 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9704\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.0837 - val_sparse_categorical_accuracy: 0.9738\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0679 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.0715 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0669 - val_sparse_categorical_accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0675 - val_sparse_categorical_accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f712ecc2b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, \n",
    "          train_labels,  \n",
    "          epochs=5,\n",
    "          validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca8c79c-9139-4cfc-b8bd-4bd0f884288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.8419833, -10.383324 ,  -3.9259145,   1.4922768, -18.739622 ,\n",
       "         -5.246328 , -19.053064 ,  12.755894 ,  -6.6638937,  -3.5348623]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = test_images[:1]\n",
    "model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82f7555-0eef-4a01-9633-df628d65b3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(test_img.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc20075-d0ac-4449-9418-ce49c664e361",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2d928-ccba-41b3-b271-0d13401c65a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(\"rm -rf mnist_model\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0485b94-58ae-4469-9538-08cb2edd0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:59:19.988144: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model.save('mnist_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c71f98-d280-4ce2-97ec-1fa9941630c2",
   "metadata": {},
   "source": [
    "### Inspect saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f7826e-897e-4fa0-a844-786902ee8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_model\n",
      "├── assets\n",
      "├── keras_metadata.pb\n",
      "├── saved_model.pb\n",
      "└── variables\n",
      "    ├── variables.data-00000-of-00001\n",
      "    └── variables.index\n",
      "\n",
      "2 directories, 4 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(\"tree mnist_model\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5eefe70-f0c9-405b-adec-aa433fdfd8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dense_input'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 784)\n",
      "      name: serving_default_dense_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(\"saved_model_cli show --dir mnist_model --tag_set serve --signature_def serving_default\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a561ba-378c-43d9-9d70-a2531fcd549e",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c59a3c5-88d6-41f6-9133-1c9a84cd2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('mnist_model')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef507e7-1931-4bc1-99d3-f4107b948918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.8419833, -10.383324 ,  -3.9259145,   1.4922768, -18.739622 ,\n",
       "         -5.246328 , -19.053064 ,  12.755894 ,  -6.6638937,  -3.5348623]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(test_images[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fdd1a-8192-454c-9eca-93e8a4b32f20",
   "metadata": {},
   "source": [
    "## MLFlow PyFunc Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013e5c2-ea4e-41e8-836b-d34d93b8faa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save as MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4493efcd-a9c0-4e6d-a6d7-8b4a28d9999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7aef8d2-85c3-4971-a6c5-78913b8af3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature, ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5623c3-8cf0-4ae1-92e8-f8bb90b8ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85bd02-be2c-4c76-ab7e-2b3529ee9232",
   "metadata": {},
   "source": [
    "#### Inferred signature (without input names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82890b00-f652-417c-a7fb-e10a7f946d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature = infer_signature(train_images, model.predict(train_images))\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ea260d0-7292-4694-84b4-9881a451e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:00:24 INFO mlflow.tensorflow: Validating the specified TensorFlow model by attempting to load it in a new TensorFlow graph...\n",
      "2022/04/26 15:00:24 INFO mlflow.tensorflow: Validation succeeded!\n"
     ]
    }
   ],
   "source": [
    "subprocess.call(\"rm -rf mnist_mlflow_infer\".split())\n",
    "mlflow.tensorflow.save_model(tf_saved_model_dir=\"mnist_model\", \n",
    "                             tf_meta_graph_tags=[\"serve\"], \n",
    "                             tf_signature_def_key=\"serving_default\",\n",
    "                             signature=signature,\n",
    "                             path=\"mnist_mlflow_infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce42485-0c83-42b8-836b-d5cd04d84a02",
   "metadata": {},
   "source": [
    "#### Manual signature (with input names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c3f4623-9662-42d9-ad8b-c010768a465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['dense_input': Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  ['dense_1': Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 784), \"dense_input\")])\n",
    "output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10), \"dense_1\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90866987-803b-4da6-8867-54d9a3a7e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:00:40 INFO mlflow.tensorflow: Validating the specified TensorFlow model by attempting to load it in a new TensorFlow graph...\n",
      "2022/04/26 15:00:40 INFO mlflow.tensorflow: Validation succeeded!\n"
     ]
    }
   ],
   "source": [
    "subprocess.call(\"rm -rf mnist_mlflow_manual\".split())\n",
    "mlflow.tensorflow.save_model(tf_saved_model_dir=\"mnist_model\", \n",
    "                             tf_meta_graph_tags=[\"serve\"], \n",
    "                             tf_signature_def_key=\"serving_default\",\n",
    "                             signature=signature,\n",
    "                             path=\"mnist_mlflow_manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca34c56-e91c-4a3f-951d-fc9bb2a64958",
   "metadata": {},
   "source": [
    "### Load data as pandas.DataFrame of 784 floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3af4ae3-9470-497b-a0c5-da771f040e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# load dataset as numpy arrays\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a0f8b44-5eb7-4585-8c1f-1df67fc7f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize\n",
    "train_images = train_images.astype(np.float32).reshape(-1, 784) / 255.0\n",
    "test_images = test_images.astype(np.float32).reshape(-1, 784) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54babbab-31e1-47ad-ac57-e62edbb34c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  774  775  776  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      777  778  779  780  781  782  783  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10000 rows x 784 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_pdf = pd.DataFrame(test_images) # .astype(float)\n",
    "test_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5645f1-ac58-45e8-a667-6d2bdd40b21c",
   "metadata": {},
   "source": [
    "### Load data as pandas.DataFrame of 1 array of 784 floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef33f1a9-dbd9-475a-9584-94ced3d95236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dense_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dense_input\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "...                                                 ...\n",
       "9995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf1 = pd.DataFrame()\n",
    "test_pdf1['dense_input'] = test_pdf.values.tolist()\n",
    "test_pdf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6342a4-b3c4-4088-bbfb-3f8da0acbec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Infer using MLFlow PyFuncModel (inferred signature w/o input names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b821900-1eae-4dc4-8fa5-5527b0e11524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ec802ba-3da8-4f53-8301-55f7c500a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Tensorflow \"flavor\" is defined in the MLModel file.\n",
    "model_infer = mlflow.pyfunc.load_model(\"mnist_mlflow_infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2adce1d-cbd5-404c-b16d-0a0d680adcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flavors:\n",
      "  python_function:\n",
      "    env: conda.yaml\n",
      "    loader_module: mlflow.tensorflow\n",
      "    python_version: 3.9.10\n",
      "  tensorflow:\n",
      "    code: null\n",
      "    meta_graph_tags:\n",
      "    - serve\n",
      "    saved_model_dir: tfmodel\n",
      "    signature_def_key: serving_default\n",
      "mlflow_version: 1.25.2.dev0\n",
      "model_uuid: 04523f5ffcc447579d728dd1055a7aea\n",
      "signature:\n",
      "  inputs: '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"float32\", \"shape\": [-1, 784]}}]'\n",
      "  outputs: '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"float32\", \"shape\": [-1,\n",
      "    10]}}]'\n",
      "utc_time_created: '2022-04-26 22:00:24.632879'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_infer.metadata)  # contents of MLModel file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41ad8c-bd98-4692-a8ce-2df02eafd8a2",
   "metadata": {},
   "source": [
    "#### Infer using pandas.DataFrame (784 floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "163beffd-4ddf-4887-98ee-291f336d78cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only dict and DataFrame input types are supported, got: <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_infer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m preds\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:631\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     data \u001b[38;5;241m=\u001b[39m _enforce_schema(data, input_schema)\n\u001b[0;32m--> 631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/tensorflow/__init__.py:538\u001b[0m, in \u001b[0;36m_TF2Wrapper.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    536\u001b[0m         feed_dict[df_col_name] \u001b[38;5;241m=\u001b[39m tensorflow\u001b[38;5;241m.\u001b[39mconstant(val)\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly dict and DataFrame input types are supported, got: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(data)))\n\u001b[1;32m    540\u001b[0m raw_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfeed_dict)\n\u001b[1;32m    541\u001b[0m pred_dict \u001b[38;5;241m=\u001b[39m {col_name: raw_preds[col_name]\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m raw_preds\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[0;31mTypeError\u001b[0m: Only dict and DataFrame input types are supported, got: <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "preds = model_infer.predict(test_pdf)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57dfd80f-c0cc-4a22-940a-094586b5e30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183b787-27dd-4dd7-a6ab-9a6f25da191a",
   "metadata": {},
   "source": [
    "#### Infer using pandas.DataFrame (array of 784 floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e8469c5-d71d-482c-9914-79c0b4bab26c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Shape of input (10000, 1) does not match expected shape (-1, 784).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_infer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pdf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m preds\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:630\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    628\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_impl\u001b[38;5;241m.\u001b[39mpredict(data)\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:585\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_actual_columns \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel inference is missing inputs. The model signature declares \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m inputs  but the provided value only has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m inputs. Note: the inputs were not named in the signature so we can \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly verify their count.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs), num_actual_columns)\n\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 585\u001b[0m     \u001b[43m_enforce_tensor_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mis_tensor_spec()\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _enforce_col_schema(pfInput, input_schema)\n\u001b[1;32m    588\u001b[0m )\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:509\u001b[0m, in \u001b[0;36m_enforce_tensor_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pfInput, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 509\u001b[0m         new_pfInput \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_tensor_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfInput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pfInput, (np\u001b[38;5;241m.\u001b[39mndarray, csc_matrix, csr_matrix)):\n\u001b[1;32m    511\u001b[0m         new_pfInput \u001b[38;5;241m=\u001b[39m _enforce_tensor_spec(pfInput, input_schema\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:453\u001b[0m, in \u001b[0;36m_enforce_tensor_spec\u001b[0;34m(values, tensor_spec)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expected \u001b[38;5;241m!=\u001b[39m actual:\n\u001b[0;32m--> 453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    454\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not match expected shape \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    455\u001b[0m                 actual_shape, expected_shape\n\u001b[1;32m    456\u001b[0m             )\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_tensor_type(actual_type) \u001b[38;5;241m!=\u001b[39m tensor_spec\u001b[38;5;241m.\u001b[39mtype:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype of input \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not match expected dtype \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    461\u001b[0m             values\u001b[38;5;241m.\u001b[39mdtype, tensor_spec\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    463\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Shape of input (10000, 1) does not match expected shape (-1, 784)."
     ]
    }
   ],
   "source": [
    "preds = model_infer.predict(test_pdf1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c287b-9076-4dda-8c70-f6c641546cc1",
   "metadata": {},
   "source": [
    "#### Infer using dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f357f551-f310-484b-906f-df7e79cbafb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "This model contains a tensor-based model signature with no input names, which suggests a numpy array input, but an input of type <class 'dict'> was found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_infer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdense_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:630\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    628\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_impl\u001b[38;5;241m.\u001b[39mpredict(data)\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:585\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_actual_columns \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel inference is missing inputs. The model signature declares \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m inputs  but the provided value only has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m inputs. Note: the inputs were not named in the signature so we can \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly verify their count.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs), num_actual_columns)\n\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 585\u001b[0m     \u001b[43m_enforce_tensor_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mis_tensor_spec()\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _enforce_col_schema(pfInput, input_schema)\n\u001b[1;32m    588\u001b[0m )\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:513\u001b[0m, in \u001b[0;36m_enforce_tensor_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    511\u001b[0m         new_pfInput \u001b[38;5;241m=\u001b[39m _enforce_tensor_spec(pfInput, input_schema\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model contains a tensor-based model signature with no input names,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which suggests a numpy array input, but an input of type \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(pfInput))\n\u001b[1;32m    517\u001b[0m         )\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_pfInput\n",
      "\u001b[0;31mMlflowException\u001b[0m: This model contains a tensor-based model signature with no input names, which suggests a numpy array input, but an input of type <class 'dict'> was found."
     ]
    }
   ],
   "source": [
    "preds = model_infer.predict({\"dense_input\": test_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf59818-65d5-4771-b2a2-19f84d53bf34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mpreds\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m result\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "result = np.array(preds['dense_1'])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b62c358-e75c-4dcb-bb35-81cc42eb6401",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpreds\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(preds['dense_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89fd26-0508-4e1e-a172-09987a2d5b1c",
   "metadata": {},
   "source": [
    "### Infer using MLFlow PyFuncModel (manual signature w/ input names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec42f8b6-bd8f-4e1c-9b81-46710fa5ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Tensorflow \"flavor\" is defined in the MLModel file.\n",
    "model_manual = mlflow.pyfunc.load_model(\"mnist_mlflow_manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2ed8d8d-a649-4dbc-823c-0fc4ed3e2655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flavors:\n",
      "  python_function:\n",
      "    env: conda.yaml\n",
      "    loader_module: mlflow.tensorflow\n",
      "    python_version: 3.9.10\n",
      "  tensorflow:\n",
      "    code: null\n",
      "    meta_graph_tags:\n",
      "    - serve\n",
      "    saved_model_dir: tfmodel\n",
      "    signature_def_key: serving_default\n",
      "mlflow_version: 1.25.2.dev0\n",
      "model_uuid: 91060fb595e34bf6978955da88a9d65e\n",
      "signature:\n",
      "  inputs: '[{\"name\": \"dense_input\", \"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"float32\",\n",
      "    \"shape\": [-1, 784]}}]'\n",
      "  outputs: '[{\"name\": \"dense_1\", \"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"float32\",\n",
      "    \"shape\": [-1, 10]}}]'\n",
      "utc_time_created: '2022-04-26 22:00:40.604597'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_manual.metadata)  # contents of MLModel file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e24ba-b433-44ee-bf4d-9a12680d1d1e",
   "metadata": {},
   "source": [
    "#### Infer using pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9753776-5134-49b6-ad03-efef88df2eb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Model is missing inputs ['dense_input']. Note that there were extra inputs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_manual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m preds\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:630\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    628\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_impl\u001b[38;5;241m.\u001b[39mpredict(data)\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:569\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    567\u001b[0m     extra_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m actual_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m extra_cols]\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[0;32m--> 569\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is missing inputs \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Note that there were extra inputs: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(missing_cols, extra_cols)\n\u001b[1;32m    572\u001b[0m         )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mis_tensor_spec():\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# The model signature does not specify column names => we can only verify column count.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     num_actual_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pfInput\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mMlflowException\u001b[0m: Model is missing inputs ['dense_input']. Note that there were extra inputs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783]"
     ]
    }
   ],
   "source": [
    "preds = model_manual.predict(test_pdf)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609979e-beed-4e02-be70-271fecc05d63",
   "metadata": {},
   "source": [
    "#### Infer using pandas.DataFrame (array of 784 floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5850a324-e367-412a-9e15-2490570195ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_manual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pdf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m preds\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:630\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    628\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_impl\u001b[38;5;241m.\u001b[39mpredict(data)\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:585\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_actual_columns \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel inference is missing inputs. The model signature declares \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m inputs  but the provided value only has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m inputs. Note: the inputs were not named in the signature so we can \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly verify their count.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs), num_actual_columns)\n\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 585\u001b[0m     \u001b[43m_enforce_tensor_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mis_tensor_spec()\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _enforce_col_schema(pfInput, input_schema)\n\u001b[1;32m    588\u001b[0m )\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:499\u001b[0m, in \u001b[0;36m_enforce_tensor_schema\u001b[0;34m(pfInput, input_schema)\u001b[0m\n\u001b[1;32m    496\u001b[0m     new_pfInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_name, tensor_spec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minput_names(), input_schema\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    498\u001b[0m         new_pfInput[col_name] \u001b[38;5;241m=\u001b[39m _enforce_tensor_spec(\n\u001b[0;32m--> 499\u001b[0m             \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfInput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m, tensor_spec\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model contains a tensor-based model signature with input names, which\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m suggests a dictionary input mapping input name to tensor, but an input of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m was found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(pfInput))\n\u001b[1;32m    506\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/envs/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py:872\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "preds = model_manual.predict(test_pdf1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6122304-a521-4a0d-8bfc-14295405783e",
   "metadata": {},
   "source": [
    "#### Infer using dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b04c3bf0-6136-4605-9634-8d0c2b68a02c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model_manual.predict({\"dense_input\": test_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a06183ea-51c9-4847-8ae2-f8fd4b895eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds[\"dense_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ef29a4d-f6c7-4ad2-a27a-4f27d7cc8620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.array(preds['dense_1'])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13c75b4f-95af-467e-956f-c2b24ffb70e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.841984</td>\n",
       "      <td>-10.383325</td>\n",
       "      <td>-3.925916</td>\n",
       "      <td>1.492277</td>\n",
       "      <td>-18.739620</td>\n",
       "      <td>-5.246327</td>\n",
       "      <td>-19.053066</td>\n",
       "      <td>12.755895</td>\n",
       "      <td>-6.663895</td>\n",
       "      <td>-3.534862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.938910</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>14.855471</td>\n",
       "      <td>-3.345494</td>\n",
       "      <td>-22.005724</td>\n",
       "      <td>-8.329589</td>\n",
       "      <td>-8.816965</td>\n",
       "      <td>-17.721798</td>\n",
       "      <td>-4.842685</td>\n",
       "      <td>-17.681454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-12.153997</td>\n",
       "      <td>6.292188</td>\n",
       "      <td>-4.426025</td>\n",
       "      <td>-6.096717</td>\n",
       "      <td>-4.840297</td>\n",
       "      <td>-7.875722</td>\n",
       "      <td>-5.291264</td>\n",
       "      <td>-2.292618</td>\n",
       "      <td>-0.789819</td>\n",
       "      <td>-9.625733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.408954</td>\n",
       "      <td>-11.872353</td>\n",
       "      <td>-1.197140</td>\n",
       "      <td>-9.336616</td>\n",
       "      <td>-7.377591</td>\n",
       "      <td>-2.899515</td>\n",
       "      <td>-1.382964</td>\n",
       "      <td>-1.484285</td>\n",
       "      <td>-11.558947</td>\n",
       "      <td>-4.084096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.335523</td>\n",
       "      <td>-13.371641</td>\n",
       "      <td>-2.976302</td>\n",
       "      <td>-9.339242</td>\n",
       "      <td>10.150408</td>\n",
       "      <td>-4.679741</td>\n",
       "      <td>-4.731390</td>\n",
       "      <td>-2.726823</td>\n",
       "      <td>-5.452237</td>\n",
       "      <td>1.658148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-12.560976</td>\n",
       "      <td>-1.898262</td>\n",
       "      <td>15.466688</td>\n",
       "      <td>0.245878</td>\n",
       "      <td>-30.197079</td>\n",
       "      <td>-5.165746</td>\n",
       "      <td>-14.938893</td>\n",
       "      <td>-1.622084</td>\n",
       "      <td>-3.197143</td>\n",
       "      <td>-13.300212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-14.929013</td>\n",
       "      <td>-4.907559</td>\n",
       "      <td>-2.158203</td>\n",
       "      <td>10.304011</td>\n",
       "      <td>-20.291252</td>\n",
       "      <td>4.632127</td>\n",
       "      <td>-24.570541</td>\n",
       "      <td>-9.770651</td>\n",
       "      <td>-4.918324</td>\n",
       "      <td>4.263149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-19.601368</td>\n",
       "      <td>-12.936716</td>\n",
       "      <td>-15.346132</td>\n",
       "      <td>-8.619569</td>\n",
       "      <td>14.286592</td>\n",
       "      <td>-10.463679</td>\n",
       "      <td>-13.707807</td>\n",
       "      <td>2.295944</td>\n",
       "      <td>-3.415435</td>\n",
       "      <td>4.011699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-4.855701</td>\n",
       "      <td>-10.713496</td>\n",
       "      <td>-10.167618</td>\n",
       "      <td>-5.768750</td>\n",
       "      <td>-11.686323</td>\n",
       "      <td>14.755033</td>\n",
       "      <td>-5.110606</td>\n",
       "      <td>-8.004922</td>\n",
       "      <td>2.898157</td>\n",
       "      <td>-13.055156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-9.965701</td>\n",
       "      <td>-11.184324</td>\n",
       "      <td>-3.866964</td>\n",
       "      <td>-7.490730</td>\n",
       "      <td>-4.589930</td>\n",
       "      <td>1.245173</td>\n",
       "      <td>14.290242</td>\n",
       "      <td>-14.960836</td>\n",
       "      <td>-5.177461</td>\n",
       "      <td>-12.376483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "0     -5.841984 -10.383325  -3.925916   1.492277 -18.739620  -5.246327   \n",
       "1    -12.938910   0.710145  14.855471  -3.345494 -22.005724  -8.329589   \n",
       "2    -12.153997   6.292188  -4.426025  -6.096717  -4.840297  -7.875722   \n",
       "3     11.408954 -11.872353  -1.197140  -9.336616  -7.377591  -2.899515   \n",
       "4     -7.335523 -13.371641  -2.976302  -9.339242  10.150408  -4.679741   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "9995 -12.560976  -1.898262  15.466688   0.245878 -30.197079  -5.165746   \n",
       "9996 -14.929013  -4.907559  -2.158203  10.304011 -20.291252   4.632127   \n",
       "9997 -19.601368 -12.936716 -15.346132  -8.619569  14.286592 -10.463679   \n",
       "9998  -4.855701 -10.713496 -10.167618  -5.768750 -11.686323  14.755033   \n",
       "9999  -9.965701 -11.184324  -3.866964  -7.490730  -4.589930   1.245173   \n",
       "\n",
       "              6          7          8          9  \n",
       "0    -19.053066  12.755895  -6.663895  -3.534862  \n",
       "1     -8.816965 -17.721798  -4.842685 -17.681454  \n",
       "2     -5.291264  -2.292618  -0.789819  -9.625733  \n",
       "3     -1.382964  -1.484285 -11.558947  -4.084096  \n",
       "4     -4.731390  -2.726823  -5.452237   1.658148  \n",
       "...         ...        ...        ...        ...  \n",
       "9995 -14.938893  -1.622084  -3.197143 -13.300212  \n",
       "9996 -24.570541  -9.770651  -4.918324   4.263149  \n",
       "9997 -13.707807   2.295944  -3.415435   4.011699  \n",
       "9998  -5.110606  -8.004922   2.898157 -13.055156  \n",
       "9999  14.290242 -14.960836  -5.177461 -12.376483  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(preds['dense_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e324586-2e7e-424b-9daa-3f7a8605446d",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd89a21d-7153-4660-83b3-b9111050fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07c0bb-56b5-45f1-a67a-629fd6207877",
   "metadata": {},
   "source": [
    "### Convert numpy array to Spark DataFrame (via Pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcfb7bbe-19fa-4a9e-bb98-e5e7398eb485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array to pandas DataFrame\n",
    "test_pdf = pd.DataFrame(test_images) # .astype(float)\n",
    "test_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac2f2367-b0be-4533-a14b-12d67c4f0433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  775  776  777  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      778  779  780  781  782  783  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...   \n",
       "9995  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "9996  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "9997  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "9998  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "9999  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                                   data  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "9995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[10000 rows x 785 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87bea901-c639-4156-a70f-f09861eabeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 4.03 ms, total: 1min 9s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 784 columns of float\n",
    "df = spark.createDataFrame(test_pdf).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5330e69-1ccb-4d8b-9a1c-44c653186c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acd25a19-befd-4de4-abf2-93c9b8c6936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:43:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/04/26 15:43:38 WARN TaskSetManager: Stage 0 contains a task of very large size (4313 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"mnist_test_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b67cd6c-b790-4baf-815e-af72e6be65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 519 ms, sys: 0 ns, total: 519 ms\n",
      "Wall time: 520 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1 column of array<float>\n",
    "test_pdf['data'] = test_pdf.values.tolist()\n",
    "pdf = test_pdf[['data']]\n",
    "pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9178c5d-b4ec-4eab-9dd5-607fd98d850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"data\",ArrayType(FloatType()),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "270afbbe-0f47-4059-8063-f6e1c470895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 s, sys: 0 ns, total: 3.3 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = spark.createDataFrame(pdf, schema).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28639646-b487-4be0-be92-39caffae0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:43:48 WARN TaskSetManager: Stage 3 contains a task of very large size (4315 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.mode(\"overwrite\").parquet(\"mnist_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2508f1d6-fad3-4356-bbae-06aa79f974e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58a59cb8-09c8-4d46-80ba-bad6663b6487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(data,ArrayType(FloatType,true),true)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a3f41-8fc4-4379-ada6-881fa1f22bf2",
   "metadata": {},
   "source": [
    "## Inference using MLFlow pyfunc.spark_udf\n",
    "Note: you can restart the kernel and run from this point to simulate running in a different node or environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c4fdb-fc54-4bab-8279-369492592765",
   "metadata": {},
   "source": [
    "### 784 columns of double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2973345-e1c5-479d-bf2b-56a8d5a93ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from pyspark.sql.functions import col, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8be5804f-73ea-41bc-8a4f-c8aac72b28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"mnist_test_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5e99f94-ef78-4943-8748-bbb82be0eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91c614-5acf-4e1c-9ec9-ead56ba1c30e",
   "metadata": {},
   "source": [
    "#### Inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8af4d50e-8327-431b-b503-bb6305f65802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:44:42 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n"
     ]
    }
   ],
   "source": [
    "mnist = mlflow.pyfunc.spark_udf(spark, model_uri=\"mnist_mlflow_infer\", result_type=\"array<float>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04ff0573-e853-45f9-97f5-223b50394093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b5b1cf3-c333-4064-8fd7-ad43b5de1981",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 55) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n",
      "    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 459, in _enforce_tensor_spec\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: dtype of input float64 does not match expected dtype float32\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:44:53 ERROR TaskSetManager: Task 2 in stage 7.0 failed 1 times; aborting job\n",
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 56) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 9.0 in stage 7.0 (TID 62) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 5.0 in stage 7.0 (TID 58) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 60) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:53 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 54) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 459, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: dtype of input float64 does not match expected dtype float32\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 459, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: dtype of input float64 does not match expected dtype float32\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist(struct(*columns))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17edc19d-0e68-4343-a121-4154d656c5bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:44:58 WARN TaskSetManager: Lost task 5.0 in stage 8.0 (TID 68) (192.168.86.223 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n",
      "    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:44:58 ERROR TaskSetManager: Task 5 in stage 8.0 failed 1 times; aborting job\n",
      "22/04/26 15:44:58 WARN TaskSetManager: Lost task 9.0 in stage 8.0 (TID 72) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:58 WARN TaskSetManager: Lost task 3.0 in stage 8.0 (TID 66) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:58 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 64) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 8.0 in stage 8.0 (TID 71) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 4.0 in stage 8.0 (TID 67) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 2.0 in stage 8.0 (TID 65) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 63) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 7.0 in stage 8.0 (TID 70) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:44:59 WARN TaskSetManager: Lost task 6.0 in stage 8.0 (TID 69) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist(*columns)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3c8d0d6-a368-4fc6-9305-427a9a4da91c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Cannot apply udf because no column names specified. The udf expects 1 columns with types: [Tensor('float32', (-1, 784))]. Input column names could not be inferred from the model signature (column names not found).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:1287\u001b[0m, in \u001b[0;36mspark_udf.<locals>.udf_with_default_cols\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m udf(\u001b[38;5;241m*\u001b[39minput_names)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1288\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply udf because no column names specified. The udf \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1289\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpects \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m columns with types: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Input column names could not be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred from the model signature (column names not found).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1291\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs),\n\u001b[1;32m   1292\u001b[0m                 input_schema\u001b[38;5;241m.\u001b[39minputs,\n\u001b[1;32m   1293\u001b[0m             ),\n\u001b[1;32m   1294\u001b[0m             error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1295\u001b[0m         )\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to apply udf on zero columns because no column names were \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as arguments or inferred from the model signature.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1300\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1301\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Cannot apply udf because no column names specified. The udf expects 1 columns with types: [Tensor('float32', (-1, 784))]. Input column names could not be inferred from the model signature (column names not found)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8b12-7e18-4040-a3f3-f7cb07d2488b",
   "metadata": {},
   "source": [
    "#### Manual schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "373abfbb-6f8d-447a-8db9-6c3e4c71c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:45:51 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n"
     ]
    }
   ],
   "source": [
    "mnist = mlflow.pyfunc.spark_udf(spark, model_uri=\"mnist_mlflow_manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "addd7e54-d66c-4658-b48a-70eddf997bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['dense_input': Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  ['dense_1': Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1154708-cec3-40da-aadc-e568934fe711",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 73) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 569, in _enforce_schema\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Model is missing inputs ['dense_input']. Note that there were extra inputs: ['190', '18', '263', '110', '695', '473', '563', '145', '38', '153', '335', '730', '317', '472', '377', '118', '258', '343', '48', '242', '68', '84', '141', '569', '499', '342', '177', '430', '520', '399', '307', '346', '669', '780', '562', '598', '359', '61', '203', '600', '12', '693', '617', '417', '15', '220', '618', '348', '497', '689', '67', '73', '179', '358', '769', '56', '57', '95', '142', '613', '453', '180', '272', '278', '716', '737', '163', '334', '288', '489', '47', '238', '515', '687', '739', '427', '729', '590', '767', '101', '26', '773', '323', '244', '28', '98', '521', '487', '322', '23', '539', '533', '587', '333', '426', '626', '435', '757', '405', '510', '120', '586', '437', '516', '686', '419', '66', '418', '292', '775', '135', '367', '355', '291', '332', '215', '442', '483', '117', '762', '389', '228', '78', '421', '62', '455', '5', '467', '747', '246', '231', '36', '181', '443', '357', '407', '570', '387', '528', '639', '707', '440', '526', '500', '69', '676', '376', '127', '197', '493', '782', '144', '683', '386', '338', '44', '748', '315', '99', '492', '114', '666', '86', '654', '192', '771', '448', '347', '765', '629', '254', '606', '337', '506', '32', '219', '509', '711', '287', '325', '341', '529', '571', '614', '133', '249', '531', '778', '52', '297', '605', '354', '108', '217', '623', '267', '640', '111', '558', '296', '574', '20', '777', '34', '647', '532', '620', '312', '207', '411', '589', '420', '591', '22', '388', '514', '543', '735', '663', '779', '316', '410', '107', '194', '584', '736', '55', '431', '566', '758', '327', '363', '565', '564', '53', '652', '247', '286', '725', '54', '273', '280', '727', '49', '498', '157', '90', '31', '694', '139', '428', '646', '391', '88', '713', '294', '384', '151', '759', '311', '239', '656', '268', '572', '362', '221', '567', '170', '206', '241', '439', '630', '245', '409', '140', '593', '214', '761', '451', '282', '544', '211', '413', '191', '450', '167', '691', '328', '458', '306', '648', '352', '257', '709', '530', '277', '575', '703', '176', '209', '134', '469', '643', '172', '189', '631', '80', '196', '309', '202', '366', '662', '699', '269', '549', '754', '10', '522', '684', '303', '275', '148', '491', '508', '58', '429', '547', '555', '94', '768', '4', '85', '295', '697', '659', '199', '128', '721', '2', '650', '690', '750', '616', '624', '104', '74', '781', '554', '592', '597', '261', '452', '619', '422', '638', '524', '718', '171', '783', '210', '403', '159', '237', '380', '121', '700', '1', '205', '752', '222', '392', '119', '527', '204', '375', '447', '158', '603', '481', '279', '505', '657', '541', '746', '178', '76', '17', '365', '223', '198', '169', '256', '460', '579', '672', '50', '402', '512', '382', '446', '682', '374', '494', '281', '122', '685', '595', '634', '743', '742', '240', '414', '502', '398', '552', '162', '41', '43', '29', '166', '138', '368', '360', '416', '218', '717', '568', '763', '182', '248', '594', '330', '63', '87', '252', '147', '658', '692', '216', '518', '635', '276', '536', '30', '760', '588', '232', '486', '728', '243', '125', '400', '628', '16', '103', '661', '200', '456', '496', '175', '556', '236', '637', '71', '174', '187', '719', '578', '313', '6', '93', '92', '542', '424', '83', '136', '265', '59', '89', '596', '155', '601', '154', '480', '667', '369', '226', '109', '423', '772', '612', '102', '185', '290', '604', '756', '385', '645', '186', '253', '344', '126', '143', '149', '670', '678', '213', '432', '130', '677', '305', '401', '370', '412', '396', '770', '156', '165', '339', '393', '534', '234', '641', '525', '559', '488', '19', '208', '466', '513', '314', '610', '212', '274', '81', '463', '173', '766', '632', '644', '733', '548', '582', '704', '523', '723', '436', '749', '113', '340', '160', '131', '731', '259', '195', '681', '45', '607', '96', '371', '706', '664', '193', '112', '679', '519', '705', '75', '535', '708', '364', '504', '621', '356', '633', '734', '97', '495', '65', '702', '573', '540', '581', '698', '283', '298', '14', '289', '406', '655', '225', '353', '233', '8', '37', '46', '301', '302', '462', '649', '106', '361', '627', '72', '372', '404', '438', '720', '77', '152', '308', '310', '184', '560', '726', '394', '517', '714', '660', '116', '13', '82', '0', '468', '485', '722', '576', '501', '751', '764', '580', '235', '444', '299', '611', '39', '449', '379', '395', '115', '100', '318', '425', '464', '553', '201', '300', '381', '550', '545', '3', '255', '285', '250', '351', '551', '561', '609', '490', '696', '688', '651', '161', '680', '701', '224', '712', '7', '329', '710', '753', '668', '459', '227', '674', '715', '345', '433', '441', '129', '9', '350', '774', '164', '229', '42', '776', '188', '454', '740', '732', '326', '577', '538', '25', '378', '675', '408', '461', '478', '636', '653', '21', '744', '40', '132', '266', '484', '673', '251', '264', '183', '27', '373', '260', '475', '91', '320', '123', '270', '474', '602', '622', '642', '79', '476', '583', '755', '324', '11', '557', '321', '319', '477', '60', '51', '390', '146', '537', '137', '397', '445', '724', '24', '470', '615', '665', '511', '230', '124', '608', '271', '331', '482', '349', '262', '741', '457', '671', '64', '479', '70', '507', '415', '434', '599', '465', '383', '503', '585', '738', '168', '35', '293', '105', '745', '304', '546', '625', '471', '284', '150', '336', '33']\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:45:58 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 8.0 in stage 9.0 (TID 81) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 6.0 in stage 9.0 (TID 79) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 2.0 in stage 9.0 (TID 75) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 4.0 in stage 9.0 (TID 77) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 569, in _enforce_schema\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Model is missing inputs ['dense_input']. Note that there were extra inputs: ['190', '18', '263', '110', '695', '473', '563', '145', '38', '153', '335', '730', '317', '472', '377', '118', '258', '343', '48', '242', '68', '84', '141', '569', '499', '342', '177', '430', '520', '399', '307', '346', '669', '780', '562', '598', '359', '61', '203', '600', '12', '693', '617', '417', '15', '220', '618', '348', '497', '689', '67', '73', '179', '358', '769', '56', '57', '95', '142', '613', '453', '180', '272', '278', '716', '737', '163', '334', '288', '489', '47', '238', '515', '687', '739', '427', '729', '590', '767', '101', '26', '773', '323', '244', '28', '98', '521', '487', '322', '23', '539', '533', '587', '333', '426', '626', '435', '757', '405', '510', '120', '586', '437', '516', '686', '419', '66', '418', '292', '775', '135', '367', '355', '291', '332', '215', '442', '483', '117', '762', '389', '228', '78', '421', '62', '455', '5', '467', '747', '246', '231', '36', '181', '443', '357', '407', '570', '387', '528', '639', '707', '440', '526', '500', '69', '676', '376', '127', '197', '493', '782', '144', '683', '386', '338', '44', '748', '315', '99', '492', '114', '666', '86', '654', '192', '771', '448', '347', '765', '629', '254', '606', '337', '506', '32', '219', '509', '711', '287', '325', '341', '529', '571', '614', '133', '249', '531', '778', '52', '297', '605', '354', '108', '217', '623', '267', '640', '111', '558', '296', '574', '20', '777', '34', '647', '532', '620', '312', '207', '411', '589', '420', '591', '22', '388', '514', '543', '735', '663', '779', '316', '410', '107', '194', '584', '736', '55', '431', '566', '758', '327', '363', '565', '564', '53', '652', '247', '286', '725', '54', '273', '280', '727', '49', '498', '157', '90', '31', '694', '139', '428', '646', '391', '88', '713', '294', '384', '151', '759', '311', '239', '656', '268', '572', '362', '221', '567', '170', '206', '241', '439', '630', '245', '409', '140', '593', '214', '761', '451', '282', '544', '211', '413', '191', '450', '167', '691', '328', '458', '306', '648', '352', '257', '709', '530', '277', '575', '703', '176', '209', '134', '469', '643', '172', '189', '631', '80', '196', '309', '202', '366', '662', '699', '269', '549', '754', '10', '522', '684', '303', '275', '148', '491', '508', '58', '429', '547', '555', '94', '768', '4', '85', '295', '697', '659', '199', '128', '721', '2', '650', '690', '750', '616', '624', '104', '74', '781', '554', '592', '597', '261', '452', '619', '422', '638', '524', '718', '171', '783', '210', '403', '159', '237', '380', '121', '700', '1', '205', '752', '222', '392', '119', '527', '204', '375', '447', '158', '603', '481', '279', '505', '657', '541', '746', '178', '76', '17', '365', '223', '198', '169', '256', '460', '579', '672', '50', '402', '512', '382', '446', '682', '374', '494', '281', '122', '685', '595', '634', '743', '742', '240', '414', '502', '398', '552', '162', '41', '43', '29', '166', '138', '368', '360', '416', '218', '717', '568', '763', '182', '248', '594', '330', '63', '87', '252', '147', '658', '692', '216', '518', '635', '276', '536', '30', '760', '588', '232', '486', '728', '243', '125', '400', '628', '16', '103', '661', '200', '456', '496', '175', '556', '236', '637', '71', '174', '187', '719', '578', '313', '6', '93', '92', '542', '424', '83', '136', '265', '59', '89', '596', '155', '601', '154', '480', '667', '369', '226', '109', '423', '772', '612', '102', '185', '290', '604', '756', '385', '645', '186', '253', '344', '126', '143', '149', '670', '678', '213', '432', '130', '677', '305', '401', '370', '412', '396', '770', '156', '165', '339', '393', '534', '234', '641', '525', '559', '488', '19', '208', '466', '513', '314', '610', '212', '274', '81', '463', '173', '766', '632', '644', '733', '548', '582', '704', '523', '723', '436', '749', '113', '340', '160', '131', '731', '259', '195', '681', '45', '607', '96', '371', '706', '664', '193', '112', '679', '519', '705', '75', '535', '708', '364', '504', '621', '356', '633', '734', '97', '495', '65', '702', '573', '540', '581', '698', '283', '298', '14', '289', '406', '655', '225', '353', '233', '8', '37', '46', '301', '302', '462', '649', '106', '361', '627', '72', '372', '404', '438', '720', '77', '152', '308', '310', '184', '560', '726', '394', '517', '714', '660', '116', '13', '82', '0', '468', '485', '722', '576', '501', '751', '764', '580', '235', '444', '299', '611', '39', '449', '379', '395', '115', '100', '318', '425', '464', '553', '201', '300', '381', '550', '545', '3', '255', '285', '250', '351', '551', '561', '609', '490', '696', '688', '651', '161', '680', '701', '224', '712', '7', '329', '710', '753', '668', '459', '227', '674', '715', '345', '433', '441', '129', '9', '350', '774', '164', '229', '42', '776', '188', '454', '740', '732', '326', '577', '538', '25', '378', '675', '408', '461', '478', '636', '653', '21', '744', '40', '132', '266', '484', '673', '251', '264', '183', '27', '373', '260', '475', '91', '320', '123', '270', '474', '602', '622', '642', '79', '476', '583', '755', '324', '11', '557', '321', '319', '477', '60', '51', '390', '146', '537', '137', '397', '445', '724', '24', '470', '615', '665', '511', '230', '124', '608', '271', '331', '482', '349', '262', '741', '457', '671', '64', '479', '70', '507', '415', '434', '599', '465', '383', '503', '585', '738', '168', '35', '293', '105', '745', '304', '546', '625', '471', '284', '150', '336', '33']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 569, in _enforce_schema\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Model is missing inputs ['dense_input']. Note that there were extra inputs: ['190', '18', '263', '110', '695', '473', '563', '145', '38', '153', '335', '730', '317', '472', '377', '118', '258', '343', '48', '242', '68', '84', '141', '569', '499', '342', '177', '430', '520', '399', '307', '346', '669', '780', '562', '598', '359', '61', '203', '600', '12', '693', '617', '417', '15', '220', '618', '348', '497', '689', '67', '73', '179', '358', '769', '56', '57', '95', '142', '613', '453', '180', '272', '278', '716', '737', '163', '334', '288', '489', '47', '238', '515', '687', '739', '427', '729', '590', '767', '101', '26', '773', '323', '244', '28', '98', '521', '487', '322', '23', '539', '533', '587', '333', '426', '626', '435', '757', '405', '510', '120', '586', '437', '516', '686', '419', '66', '418', '292', '775', '135', '367', '355', '291', '332', '215', '442', '483', '117', '762', '389', '228', '78', '421', '62', '455', '5', '467', '747', '246', '231', '36', '181', '443', '357', '407', '570', '387', '528', '639', '707', '440', '526', '500', '69', '676', '376', '127', '197', '493', '782', '144', '683', '386', '338', '44', '748', '315', '99', '492', '114', '666', '86', '654', '192', '771', '448', '347', '765', '629', '254', '606', '337', '506', '32', '219', '509', '711', '287', '325', '341', '529', '571', '614', '133', '249', '531', '778', '52', '297', '605', '354', '108', '217', '623', '267', '640', '111', '558', '296', '574', '20', '777', '34', '647', '532', '620', '312', '207', '411', '589', '420', '591', '22', '388', '514', '543', '735', '663', '779', '316', '410', '107', '194', '584', '736', '55', '431', '566', '758', '327', '363', '565', '564', '53', '652', '247', '286', '725', '54', '273', '280', '727', '49', '498', '157', '90', '31', '694', '139', '428', '646', '391', '88', '713', '294', '384', '151', '759', '311', '239', '656', '268', '572', '362', '221', '567', '170', '206', '241', '439', '630', '245', '409', '140', '593', '214', '761', '451', '282', '544', '211', '413', '191', '450', '167', '691', '328', '458', '306', '648', '352', '257', '709', '530', '277', '575', '703', '176', '209', '134', '469', '643', '172', '189', '631', '80', '196', '309', '202', '366', '662', '699', '269', '549', '754', '10', '522', '684', '303', '275', '148', '491', '508', '58', '429', '547', '555', '94', '768', '4', '85', '295', '697', '659', '199', '128', '721', '2', '650', '690', '750', '616', '624', '104', '74', '781', '554', '592', '597', '261', '452', '619', '422', '638', '524', '718', '171', '783', '210', '403', '159', '237', '380', '121', '700', '1', '205', '752', '222', '392', '119', '527', '204', '375', '447', '158', '603', '481', '279', '505', '657', '541', '746', '178', '76', '17', '365', '223', '198', '169', '256', '460', '579', '672', '50', '402', '512', '382', '446', '682', '374', '494', '281', '122', '685', '595', '634', '743', '742', '240', '414', '502', '398', '552', '162', '41', '43', '29', '166', '138', '368', '360', '416', '218', '717', '568', '763', '182', '248', '594', '330', '63', '87', '252', '147', '658', '692', '216', '518', '635', '276', '536', '30', '760', '588', '232', '486', '728', '243', '125', '400', '628', '16', '103', '661', '200', '456', '496', '175', '556', '236', '637', '71', '174', '187', '719', '578', '313', '6', '93', '92', '542', '424', '83', '136', '265', '59', '89', '596', '155', '601', '154', '480', '667', '369', '226', '109', '423', '772', '612', '102', '185', '290', '604', '756', '385', '645', '186', '253', '344', '126', '143', '149', '670', '678', '213', '432', '130', '677', '305', '401', '370', '412', '396', '770', '156', '165', '339', '393', '534', '234', '641', '525', '559', '488', '19', '208', '466', '513', '314', '610', '212', '274', '81', '463', '173', '766', '632', '644', '733', '548', '582', '704', '523', '723', '436', '749', '113', '340', '160', '131', '731', '259', '195', '681', '45', '607', '96', '371', '706', '664', '193', '112', '679', '519', '705', '75', '535', '708', '364', '504', '621', '356', '633', '734', '97', '495', '65', '702', '573', '540', '581', '698', '283', '298', '14', '289', '406', '655', '225', '353', '233', '8', '37', '46', '301', '302', '462', '649', '106', '361', '627', '72', '372', '404', '438', '720', '77', '152', '308', '310', '184', '560', '726', '394', '517', '714', '660', '116', '13', '82', '0', '468', '485', '722', '576', '501', '751', '764', '580', '235', '444', '299', '611', '39', '449', '379', '395', '115', '100', '318', '425', '464', '553', '201', '300', '381', '550', '545', '3', '255', '285', '250', '351', '551', '561', '609', '490', '696', '688', '651', '161', '680', '701', '224', '712', '7', '329', '710', '753', '668', '459', '227', '674', '715', '345', '433', '441', '129', '9', '350', '774', '164', '229', '42', '776', '188', '454', '740', '732', '326', '577', '538', '25', '378', '675', '408', '461', '478', '636', '653', '21', '744', '40', '132', '266', '484', '673', '251', '264', '183', '27', '373', '260', '475', '91', '320', '123', '270', '474', '602', '622', '642', '79', '476', '583', '755', '324', '11', '557', '321', '319', '477', '60', '51', '390', '146', '537', '137', '397', '445', '724', '24', '470', '615', '665', '511', '230', '124', '608', '271', '331', '482', '349', '262', '741', '457', '671', '64', '479', '70', '507', '415', '434', '599', '465', '383', '503', '585', '738', '168', '35', '293', '105', '745', '304', '546', '625', '471', '284', '150', '336', '33']\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist(struct(*columns))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4b2a01d-bb99-4a1c-9022-1e954fba4c72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 74) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 5.0 in stage 9.0 (TID 78) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 3.0 in stage 9.0 (TID 76) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 9.0 in stage 9.0 (TID 82) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:45:58 WARN TaskSetManager: Lost task 7.0 in stage 9.0 (TID 80) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 8.0 in stage 10.0 (TID 91) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 498, in _enforce_tensor_schema\n",
      "    new_pfInput[col_name] = _enforce_tensor_spec(\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 444, in _enforce_tensor_spec\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Shape of input (999,) does not match expected shape (-1, 784).\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:46:03 ERROR TaskSetManager: Task 8 in stage 10.0 failed 1 times; aborting job\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 5.0 in stage 10.0 (TID 88) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 83) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 7.0 in stage 10.0 (TID 90) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 1.0 in stage 10.0 (TID 84) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 6.0 in stage 10.0 (TID 89) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 9.0 in stage 10.0 (TID 92) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 2.0 in stage 10.0 (TID 85) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 4.0 in stage 10.0 (TID 87) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:03 WARN TaskSetManager: Lost task 3.0 in stage 10.0 (TID 86) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 498, in _enforce_tensor_schema\n    new_pfInput[col_name] = _enforce_tensor_spec(\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 444, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (999,) does not match expected shape (-1, 784).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 498, in _enforce_tensor_schema\n    new_pfInput[col_name] = _enforce_tensor_spec(\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 444, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (999,) does not match expected shape (-1, 784).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist(*columns)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e64ae441-6243-4ea9-b131-b582e15a522a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'dense_input' given input columns: [0, 1, 10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 11, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 12, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 13, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 16, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 17, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 18, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 19, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 2, 20, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 21, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 22, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 24, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 63, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 64, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 65, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 68, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 69, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 7, 70, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 71, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 72, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 75, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 76, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 77, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 78, 780, 781, 782, 783, 79, 8, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 9, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99];\n'Project [0#2355, 1#2356, 2#2357, 3#2358, 4#2359, 5#2360, 6#2361, 7#2362, 8#2363, 9#2364, 10#2365, 11#2366, 12#2367, 13#2368, 14#2369, 15#2370, 16#2371, 17#2372, 18#2373, 19#2374, 20#2375, 21#2376, 22#2377, 23#2378, ... 761 more fields]\n+- Relation [0#2355,1#2356,2#2357,3#2358,4#2359,5#2360,6#2361,7#2362,8#2363,9#2364,10#2365,11#2366,12#2367,13#2368,14#2369,15#2370,16#2371,17#2372,18#2373,19#2374,20#2375,21#2376,22#2377,23#2378,... 760 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:2478\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'dense_input' given input columns: [0, 1, 10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 11, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 12, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 13, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 16, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 17, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 18, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 19, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 2, 20, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 21, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 22, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 24, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 63, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 64, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 65, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 68, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 69, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 7, 70, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 71, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 72, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 75, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 76, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 77, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 78, 780, 781, 782, 783, 79, 8, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 9, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99];\n'Project [0#2355, 1#2356, 2#2357, 3#2358, 4#2359, 5#2360, 6#2361, 7#2362, 8#2363, 9#2364, 10#2365, 11#2366, 12#2367, 13#2368, 14#2369, 15#2370, 16#2371, 17#2372, 18#2373, 19#2374, 20#2375, 21#2376, 22#2377, 23#2378, ... 761 more fields]\n+- Relation [0#2355,1#2356,2#2357,3#2358,4#2359,5#2360,6#2361,7#2362,8#2363,9#2364,10#2365,11#2366,12#2367,13#2368,14#2369,15#2370,16#2371,17#2372,18#2373,19#2374,20#2375,21#2376,22#2377,23#2378,... 760 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df.withColumn(\"preds\", mnist()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58303006-8da6-460e-b988-3cc3b06148c8",
   "metadata": {},
   "source": [
    "### 1 column of 784 float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfbc0feb-d281-4aa4-af62-8a5825e51358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4314fb2-ffee-47dd-8718-053139a61ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"mnist_test\").withColumnRenamed(\"data\", \"dense_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74661d24-ac7e-4e7e-b85d-51f50f239b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_input']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df1.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5960a3e-b9f5-42b0-9b8c-fb5e04a1b871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dense_input,ArrayType(FloatType,true),true)))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989ba9e-c570-49f5-af5d-e69babd3538d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e9f9111-0b7d-4aa5-9b67-f701b837d580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:46:47 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n"
     ]
    }
   ],
   "source": [
    "mnist_infer = mlflow.pyfunc.spark_udf(spark, model_uri=\"mnist_mlflow_infer\", result_type=\"array<float>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b75ca65-cce5-41b1-9af8-1673c20a2084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_infer.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7ca82d2-92a0-4741-a59e-97e38f91cc58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 4.0 in stage 12.0 (TID 98) (192.168.86.223 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n",
      "    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:46:51 ERROR TaskSetManager: Task 4 in stage 12.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (1000, 1) does not match expected shape (-1, 784).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist_infer(struct(*columns))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9fc9944-9db3-434c-a77d-6afcb38f352a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 94) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 1.0 in stage 12.0 (TID 95) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 6.0 in stage 12.0 (TID 100) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 8.0 in stage 12.0 (TID 102) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 7.0 in stage 12.0 (TID 101) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 9.0 in stage 12.0 (TID 103) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 5.0 in stage 12.0 (TID 99) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 2.0 in stage 12.0 (TID 96) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:51 WARN TaskSetManager: Lost task 3.0 in stage 12.0 (TID 97) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 3.0 in stage 13.0 (TID 107) (192.168.86.223 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n",
      "    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Shape of input (999, 1) does not match expected shape (-1, 784).\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:46:54 ERROR TaskSetManager: Task 3 in stage 13.0 failed 1 times; aborting job\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 1.0 in stage 13.0 (TID 105) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 104) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 2.0 in stage 13.0 (TID 106) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 6.0 in stage 13.0 (TID 110) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 4.0 in stage 13.0 (TID 108) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 5.0 in stage 13.0 (TID 109) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 7.0 in stage 13.0 (TID 111) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 9.0 in stage 13.0 (TID 113) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:54 WARN TaskSetManager: Lost task 8.0 in stage 13.0 (TID 112) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (999, 1) does not match expected shape (-1, 784).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 509, in _enforce_tensor_schema\n    new_pfInput = _enforce_tensor_spec(pfInput.to_numpy(), input_schema.inputs[0])\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 453, in _enforce_tensor_spec\n    raise MlflowException(\nmlflow.exceptions.MlflowException: Shape of input (999, 1) does not match expected shape (-1, 784).\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist_infer(*columns)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76f9600c-9ff2-4060-9723-bedd003786dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Cannot apply udf because no column names specified. The udf expects 1 columns with types: [Tensor('float32', (-1, 784))]. Input column names could not be inferred from the model signature (column names not found).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/mlflow/mlflow/pyfunc/__init__.py:1287\u001b[0m, in \u001b[0;36mspark_udf.<locals>.udf_with_default_cols\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m udf(\u001b[38;5;241m*\u001b[39minput_names)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1288\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply udf because no column names specified. The udf \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1289\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpects \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m columns with types: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Input column names could not be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred from the model signature (column names not found).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1291\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs),\n\u001b[1;32m   1292\u001b[0m                 input_schema\u001b[38;5;241m.\u001b[39minputs,\n\u001b[1;32m   1293\u001b[0m             ),\n\u001b[1;32m   1294\u001b[0m             error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1295\u001b[0m         )\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to apply udf on zero columns because no column names were \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as arguments or inferred from the model signature.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1300\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1301\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Cannot apply udf because no column names specified. The udf expects 1 columns with types: [Tensor('float32', (-1, 784))]. Input column names could not be inferred from the model signature (column names not found)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist_infer()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942aa0b-6515-4229-b45b-779abdee0d66",
   "metadata": {},
   "source": [
    "#### Manual schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ece058f9-ae99-4753-80ce-ecc3785e0556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/26 15:46:55 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n"
     ]
    }
   ],
   "source": [
    "mnist = mlflow.pyfunc.spark_udf(spark, model_uri=\"mnist_mlflow_manual\", result_type=\"array<float>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c468b9fe-91fe-47a9-9273-da4f77bbc7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['dense_input': Tensor('float32', (-1, 784))]\n",
       "outputs: \n",
       "  ['dense_1': Tensor('float32', (-1, 10))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e4f2879-ad13-4b4c-9f88-ea4cdcc109cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 3.0 in stage 14.0 (TID 117) (192.168.86.223 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n",
      "    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n",
      "  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:46:58 ERROR TaskSetManager: Task 3 in stage 14.0 failed 1 times; aborting job\n",
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 114) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 4.0 in stage 14.0 (TID 118) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 5.0 in stage 14.0 (TID 119) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 2.0 in stage 14.0 (TID 116) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 8.0 in stage 14.0 (TID 122) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist(struct(*columns))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fabea9d8-92a7-4f3d-9425-3911703b64b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:46:58 WARN TaskSetManager: Lost task 7.0 in stage 14.0 (TID 121) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 3.0 in stage 15.0 (TID 127) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n",
      "    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n",
      "  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:47:01 ERROR TaskSetManager: Task 3 in stage 15.0 failed 1 times; aborting job\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 7.0 in stage 15.0 (TID 131) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 9.0 in stage 15.0 (TID 133) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 1.0 in stage 15.0 (TID 125) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 8.0 in stage 15.0 (TID 132) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 124) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 6.0 in stage 15.0 (TID 130) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist(*columns)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea8d1f30-e02a-4a4c-9829-e54514964dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:47:01 WARN TaskSetManager: Lost task 4.0 in stage 15.0 (TID 128) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:04 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 134) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n",
      "    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n",
      "    result = predict_fn(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n",
      "    return loaded_model.predict(pdf)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n",
      "    data = _enforce_schema(data, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n",
      "    _enforce_tensor_schema(pfInput, input_schema)\n",
      "  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n",
      "    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n",
      "  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/04/26 15:47:04 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n",
      "22/04/26 15:47:04 WARN TaskSetManager: Lost task 5.0 in stage 16.0 (TID 139) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:05 WARN TaskSetManager: Lost task 1.0 in stage 16.0 (TID 135) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:05 WARN TaskSetManager: Lost task 6.0 in stage 16.0 (TID 140) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:05 WARN TaskSetManager: Lost task 4.0 in stage 16.0 (TID 138) (192.168.86.223 executor 0): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:05 WARN TaskSetManager: Lost task 7.0 in stage 16.0 (TID 141) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n",
      "22/04/26 15:47:05 WARN TaskSetManager: Lost task 3.0 in stage 16.0 (TID 137) (192.168.86.223 executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/devpub/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1273, in udf\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1115, in _predict_row_batch\n    result = predict_fn(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 1255, in batch_predict_fn\n    return loaded_model.predict(pdf)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 630, in predict\n    data = _enforce_schema(data, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 585, in _enforce_schema\n    _enforce_tensor_schema(pfInput, input_schema)\n  File \"/home/leey/devpub/mlflow/mlflow/pyfunc/__init__.py\", line 499, in _enforce_tensor_schema\n    np.array(pfInput[col_name], dtype=tensor_spec.type), tensor_spec\n  File \"/home/leey/.pyenv/versions/sparkext_mlflow/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n    return np.asarray(self._values, dtype)\nValueError: setting an array element with a sequence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 15:47:31 WARN Dispatcher: Message RemoteProcessDisconnected(192.168.86.223:41542) dropped. Could not find BlockManagerMasterHeartbeat.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leey/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/leey/devpub/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = df1.withColumn(\"preds\", mnist()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd81eb-f271-40b4-8d63-7cf5d2d9ee7c",
   "metadata": {},
   "source": [
    "### Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380f5d7-f22e-428f-9e9a-23d6c00413a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0].preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e38918-e7f7-4df3-808f-a546fe09a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c340fdd-b0a1-4c8e-b2d4-f898dd9e79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_pdf.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789386a-4ebd-433d-857f-93cbbefb7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(preds[0].dense_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587db618-3477-40d2-b17a-0bc112574803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img.reshape(28,28))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
